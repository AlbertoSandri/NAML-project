{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVzKF_-2woHf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udowETk_skjj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import Counter\n",
        "from sklearn.metrics import roc_curve, auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dPE2EZW7QkB"
      },
      "source": [
        "Link to the [dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4A5CclDpNWB"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('drive/MyDrive/creditcard.csv', delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTxDeFQ3zxAb"
      },
      "source": [
        "## Data inspection\n",
        "\n",
        "Display some basic information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-KPQ-i3tL1s"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cy1e56i66hWo"
      },
      "outputs": [],
      "source": [
        "n_samples = data.shape[0]\n",
        "n_features = data.shape[1] - 1 # 'Class' is not a feature\n",
        "n_frauds = np.sum(data['Class'])\n",
        "\n",
        "print('%d samples' %n_samples)\n",
        "print('%d features' %n_features)\n",
        "print('%d frauds' %n_frauds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FI-ZRGZzXZo"
      },
      "outputs": [],
      "source": [
        "data.info() # print datatypes, there aren't null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MV4hRT51zZ_4"
      },
      "outputs": [],
      "source": [
        "data.describe() #basic statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jio8rtx05YaI"
      },
      "source": [
        "## Attribute vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpRuQjoR09rY"
      },
      "outputs": [],
      "source": [
        "# Make the attribute vectors with the GA selected features\n",
        "v1 = ['V1', 'V5', 'V7', 'V8', 'V11', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'Amount', 'Class'] # 'Class' added\n",
        "data_1 = data[v1] \n",
        "v2 = ['V1', 'V6', 'V13', 'V16', 'V17', 'V22', 'V23', 'V28', 'Amount', 'Class']\n",
        "data_2 = data[v2]\n",
        "v3 = ['V2', 'V11', 'V12', 'V13', 'V15', 'V16', 'V17', 'V18', 'V20', 'V21', 'V24', 'V26', 'Amount', 'Class']\n",
        "data_3 = data[v3]\n",
        "v4 = ['V2', 'V7', 'V10', 'V13', 'V15', 'V17', 'V19', 'V28', 'Amount', 'Class']\n",
        "data_4 = data[v4]\n",
        "v5 = ['Time', 'V1', 'V7', 'V8', 'V9', 'V11', 'V12', 'V14', 'V15', 'V22', 'V27', 'V28', 'Amount', 'Class']\n",
        "data_5 = data[v5]\n",
        "# dataset with full feature vector\n",
        "v6 = data.columns\n",
        "data_6 = data.copy()\n",
        "# dataset with random feature vector\n",
        "v_random = ['V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V11', 'V12', 'V13', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V25', 'V26', 'V28', 'Amount', 'Class']\n",
        "data_7 = data[v_random]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "0lhxqM-4odId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E92bLkY4z9P1"
      },
      "outputs": [],
      "source": [
        "data_2.corr() # this and the following plots are not readable with the entire dataset, maybe do this just for the selected features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiUgDiXE0CU3"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots(figsize=(22,10))\n",
        "sns.heatmap(data_2.corr(), annot = True, cmap = 'vlag_r', vmin = -1, vmax = 1,  ax = ax) \n",
        "# PCA was performed on V1, ..., V28 in fact all the covariances are very near to 0. Only Time and Amount were not trasformed through PCA and so they have som covariances \n",
        "# different from 0. It is possinle to see that the Class, that represents the feature we want to predict has negative coorelation with some features like V16 and V17"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(data_2, diag_kind='kde') # non molto significativo in my opinion"
      ],
      "metadata": {
        "id": "FPGC3xltWtsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mld65YMdAmQO"
      },
      "source": [
        "## Data normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g1KOqpoAq2l"
      },
      "source": [
        "Min-max scaling method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FL8PfA9yApo5"
      },
      "outputs": [],
      "source": [
        "def min_max(data, data_min=None, data_max=None):\n",
        "  '''\n",
        "  Applies the min-max scaling method to a numpy dataset\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  data : ndarray\n",
        "    dataset to scale with samples on rows\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  data_normalized : ndarray\n",
        "    original dataset but normalized\n",
        "  '''\n",
        "  if data_min is None or data_max is None:\n",
        "    data_min = data.min(axis=0)\n",
        "    data_max = data.max(axis=0)\n",
        "  data_normalized = (data - data_min[None,:]) / (data_max[None,:] - data_min[None,:]) \n",
        "  return data_normalized, data_min, data_max"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_normalized = min_max(data_2.to_numpy())\n",
        "_, ax = plt.subplots(figsize=(22,10))\n",
        "sns.violinplot(data = data_normalized, ax = ax)"
      ],
      "metadata": {
        "id": "3bB01k0HWEdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFOVCtun53bg"
      },
      "source": [
        "## Train-validation split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uyw3FgyO54Lo"
      },
      "outputs": [],
      "source": [
        "# maybe use cross-validation\n",
        "# 60% train, 20% validation, 20% test, maybe it is not necessary having the validation\n",
        "\n",
        "def train_split(data_input, percentage_train=0.6, percentage_validation=0.2):\n",
        "  '''\n",
        "  Shuffles the dataset and splits in train, validation and test\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  data_input : ndarray\n",
        "    dataset to split with samples on rows\n",
        "  percentage_train : float\n",
        "    percentage of data to put in the train dataset\n",
        "  percentage_validation : float\n",
        "    percentage of data to put in the validation dataset\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  x_train : ndarray\n",
        "    train dataset with samples\n",
        "  y_train : ndarray\n",
        "    train dataset with labels\n",
        "  x_valid : ndarray\n",
        "    validation dataset with samples\n",
        "  y_valid : ndarray\n",
        "    validation dataset with labels\n",
        "  x_test : ndarray\n",
        "    test dataset with samples\n",
        "  y_test : ndarray\n",
        "    test dataset with labels\n",
        "\n",
        "  Raises\n",
        "  ------\n",
        "  Exception : if the sum of the two percentages is greater than 1 or at least \n",
        "              one of them is negative\n",
        "  '''\n",
        "\n",
        "  # check validity of the parameters\n",
        "  if percentage_train + percentage_validation > 1 or percentage_train < 0 or\\\n",
        "    percentage_validation < 0:\n",
        "    raise Exception('Percentages must be positive and their sum must be lower \\\n",
        "                    or equal to 1')\n",
        "    \n",
        "  data = data_input.copy()\n",
        "\n",
        "  np.random.seed(0) # for reproducibility\n",
        "  np.random.shuffle(data) # shuffle along first axis so rows\n",
        "\n",
        "  # take the number of samples to put in each dataset\n",
        "  num_train = int(data.shape[0] * percentage_train) \n",
        "  num_val = num_train + int(data.shape[0] * percentage_validation) \n",
        "\n",
        "  # split the dataset\n",
        "  x_train = data[:num_train, :-1] #don't take last column that is the class\n",
        "  y_train = data[:num_train, -1:]\n",
        "  x_valid = data[num_train:num_val, :-1]\n",
        "  y_valid = data[num_train:num_val, -1:]\n",
        "  x_test =  data[num_val:, :-1] \n",
        "  y_test =  data[num_val:, -1:]\n",
        "\n",
        "  return x_train, y_train, x_valid, y_valid, x_test, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Undersampling"
      ],
      "metadata": {
        "id": "eq2mUxrLEAy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# undersample the majority class, this will create a balanced dataset, allowing the classifier to better distinguish between the two classes.\n",
        "\n",
        "def undersample(x, y, ratio):\n",
        "  '''\n",
        "  Performs undersampling on the dataset\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x : ndarray\n",
        "    dataset to undersample with samples on \n",
        "  y : ndarray\n",
        "    corresponding labels of the dataset to undersample\n",
        "  ratio : float\n",
        "    desired ratio of samples between majority and minority class.\n",
        "    It assumes that label '1' is the minority class\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  x[idxs, :] : ndarray\n",
        "    dataset undersampled with samples\n",
        "  y[idxs, :] : ndarray\n",
        "    corresponding labels of the undersampled dataset\n",
        "  '''\n",
        "\n",
        "  # take indexes corresponding to labels '0' and '1'\n",
        "  idxs_zeros = np.where(y == 0.)[0]\n",
        "  idxs_ones = np.where(y == 1.)[0]\n",
        "\n",
        "  # compute the number of samples of the majority class to keep\n",
        "  n_samples = int(len(idxs_ones) * ratio)\n",
        "\n",
        "  # extract randomly the samples from the majority class\n",
        "  idxs_zeros = np.random.choice(idxs_zeros, min(n_samples, len(idxs_zeros)), replace = False)\n",
        "  \n",
        "  # extract the undersampled dataset\n",
        "  idxs = np.concatenate((idxs_zeros, idxs_ones))\n",
        "  return x[idxs, :], y[idxs, :]"
      ],
      "metadata": {
        "id": "ImeZXMgfECgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRvGJ2sVigOT"
      },
      "source": [
        "##Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfEGq0L_VXIg"
      },
      "outputs": [],
      "source": [
        "# Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise\n",
        "\n",
        "def metrics(predictions, true_labels, axis=None):\n",
        "  '''\n",
        "  Computes and print metrics TP, TN, FP, FN, AC, RC, PC, F1\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  predictions : ndarray\n",
        "    predictions of samples obtained with a model\n",
        "  true_labels : ndarray\n",
        "    true labels of the samples\n",
        "  axis : matplotlib.axes._subplots.AxesSubplot, optional\n",
        "    to print the outputs on a box in a plot\n",
        "  '''\n",
        "\n",
        "  TP = np.sum(np.logical_and(predictions == 1., true_labels == 1.)) # attacks accurately flagged as attacks\n",
        "  TN = np.sum(np.logical_and(predictions == 0., true_labels == 0.)) # normal traffic accurately flagged as normal\n",
        "  FP = np.sum(np.logical_and(predictions == 1., true_labels == 0.)) # normal traffic incorrectly flagged as attacks\n",
        "  FN = np.sum(np.logical_and(predictions == 0., true_labels == 1.)) # attacks incorrectly flagged as normal \n",
        "\n",
        "  AC = ((TN + TP) / len(predictions)) * 100 # accuracy\n",
        "  RC = (TP / (FN + TP)) * 100 # recall or sensitivity\n",
        "  PR = (TP / (FP + TP)) * 100 # precision\n",
        "  F1 = 2 * PR * RC / (PR + RC) # F1-Score\n",
        "\n",
        "  if axis is None:\n",
        "    # print the values\n",
        "    print('TP: %d' % TP)\n",
        "    print('TN: %d' % TN)\n",
        "    print('FP: %d' % FP)\n",
        "    print('FN: %d' % FN)\n",
        "    print('accuracy: %1.2f%%' % AC)\n",
        "    print('recall: %1.2f%%' % RC)\n",
        "    print('precision: %1.2f%%' % PR)\n",
        "    print('F1-Score: %1.2f%%' % F1)\n",
        "  else:\n",
        "    # print values on a plot\n",
        "    text = 'accuracy %1.2f%%\\nrecall: %1.2f%%\\nprecision: %1.2f%%\\nF1-Score: %1.2f%%' % (AC, RC, PR, F1)\n",
        "    axis.annotate(text, xy=(0.75, 0.75), xycoords='axes fraction',\n",
        "             bbox=dict(boxstyle=\"round,pad=0.8\", fc=(.5, 1., 1.)),\n",
        "             fontsize=14, color='black')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise\n",
        "\n",
        "def metricsDF(predictions, true_labels, metrics_df=None, dataset_label='NotLabeled'):\n",
        "  \"\"\"\n",
        "  Computes and print metrics TP, TN, FP, FN, AC, RC, PC, F1\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  predictions : ndarray\n",
        "    predictions of samples obtained with a model\n",
        "  true_labels : ndarray\n",
        "    true labels of the samples\n",
        "  metrics_df: DataFrame\n",
        "    DataFrame to which the computed statistics have to be put\n",
        "  dataset_label: str\n",
        "    label identifing the belonging of the statistcs to its dataset\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  DataFrame\n",
        "    DataFrame containing the statistics contained in the parameter metrics_df\n",
        "    plus the statistics computed on the new predictions\n",
        "  \"\"\"\n",
        "\n",
        "  TP = np.sum(np.logical_and(predictions == 1., true_labels == 1.)) # attacks accurately flagged as attacks\n",
        "  TN = np.sum(np.logical_and(predictions == 0., true_labels == 0.)) # normal traffic accurately flagged as normal\n",
        "  FP = np.sum(np.logical_and(predictions == 1., true_labels == 0.)) # normal traffic incorrectly flagged as attacks\n",
        "  FN = np.sum(np.logical_and(predictions == 0., true_labels == 1.)) # attacks incorrectly flagged as normal \n",
        "\n",
        "  AC = ((TN + TP) / len(predictions)) * 100 # accuracy\n",
        "  RC = (TP / (FN + TP)) * 100 # recall or sensitivity\n",
        "  PR = (TP / (FP + TP)) * 100 # precision\n",
        "  F1 = 2 * PR * RC / (PR + RC) # F1-Score\n",
        "\n",
        "  if metrics_df is None:\n",
        "    columns = ['Set of features', 'TP', 'TN', 'FP', 'FN', 'accuracy', 'recall', 'precision', 'F1-score']\n",
        "    metrics_df = pd.DataFrame([[dataset_label, TP, TN, FP, FN, AC, RC, PR, F1]], columns=columns)\n",
        "  else:\n",
        "    columns = ['Set of features', 'TP', 'TN', 'FP', 'FN', 'accuracy', 'recall', 'precision', 'F1-score']\n",
        "    metrics_df = metrics_df.append(pd.DataFrame([[dataset_label, TP, TN, FP, FN, AC, RC, PR, F1]], columns=columns))\n",
        "  \n",
        "  return metrics_df"
      ],
      "metadata": {
        "id": "2LMipb1OHRj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9DA4mud_MUw"
      },
      "source": [
        "##Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KyXk2vEHYIA"
      },
      "outputs": [],
      "source": [
        "# can add more loss, like accuracy and MSE(se vuoi sotto in ANN sono definiti, ma con params = [bias, weights] e quind x = [1,x] o simile)\n",
        "# penalize more error on frauds not detected, so the model pays more attention to this minority class\n",
        "\n",
        "def MSE(x, y, weights, bias):\n",
        "  '''\n",
        "  Computes the mean squared error.\n",
        "  '''\n",
        "\n",
        "  # compute prediction\n",
        "  y_pred = 1 / (1 + jnp.exp(-(bias + x @ weights)))\n",
        "  # compute loss\n",
        "  return jnp.mean(jnp.square(y - y_pred))\n",
        "\n",
        "def cross_entropy(x, y, weights, bias):\n",
        "  '''\n",
        "  Cross entropy cost function\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x : ndarray\n",
        "    samples\n",
        "  y : ndarray\n",
        "    true labels associated to samples\n",
        "  weights : ndarray\n",
        "    weights of the model\n",
        "  bias : float\n",
        "    bias of the model\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  ndarray\n",
        "    the value of the cost function with the given input\n",
        "  '''\n",
        "\n",
        "  # compute prediction\n",
        "  y_pred = 1 / (1 + jnp.exp(-(bias + x @ weights)))\n",
        "  # compute loss\n",
        "  return - jnp.mean(2. * y * jnp.log(y_pred) + 0.5 * (1-y) * jnp.log(1-y_pred)) \n",
        "\n",
        "# gradient computation and jit compilation of the cost function\n",
        "grad = jax.grad(cross_entropy, argnums = [2,3])\n",
        "grad_jit = jax.jit(grad)\n",
        "cross_entropy_jit = jax.jit(cross_entropy)\n",
        "\n",
        "grad = jax.grad(MSE, argnums = [2,3])\n",
        "grad_MSE_jit = jax.jit(grad)\n",
        "MSE_jit = jax.jit(MSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A08AjL0L_LzX"
      },
      "outputs": [],
      "source": [
        "# using vanilla SGD, so without learning rate decay and batch size doesn't work\n",
        "# tried RMSPROP, doesn't work well\n",
        "# we could add the callback\n",
        "\n",
        "def SGD(x_train, y_train, x_valid=None, y_valid = None, num_epochs=50000, learning_rate_max=1e-1, \\\n",
        "        learning_rate_min = 1e-3, learning_rate_decay = 50000, batch_size = 32):\n",
        "  '''\n",
        "  SGD method with mini-batch and learning rate decay\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x_train : ndarray\n",
        "    train dataset with samples on rows\n",
        "  y_train : ndarray\n",
        "    train labels\n",
        "  x_train : ndarray, optional\n",
        "    validation dataset with samples on rows\n",
        "  y_train : ndarray, optional\n",
        "    validation labels\n",
        "  num_epochs : int\n",
        "    number of epochs to perform (default is 50000)\n",
        "  learning_rate_max : float\n",
        "    max value of the learning rate (default is 1e-1)\n",
        "  learning_rate_min : float\n",
        "    min value of the learning rate (default is 1e-3)\n",
        "  learning_rate_decay : int\n",
        "    number of epochs to reach the learning_rate_min (default is 50000)\n",
        "  batch_size : int\n",
        "    size of the batch (default is 32)\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  weights : ndarray\n",
        "    computed weights of the model\n",
        "  bias : float\n",
        "    computed bias of the model\n",
        "  history : list\n",
        "    list with the history of the cost function values on training dataset\n",
        "  '''\n",
        "\n",
        "  # check if there is a validation dataset\n",
        "  validation = True if x_valid is not None and y_valid is not None else False\n",
        "\n",
        "  # initialize weights and bias\n",
        "  weights = np.random.randn(x_train.shape[1], 1) \n",
        "  bias = 0.0\n",
        "\n",
        "  # initialize history of the cost function\n",
        "  history = list()\n",
        "  history.append(cross_entropy_jit(x_train, y_train, weights, bias))\n",
        "  if validation:\n",
        "    history_valid = list()\n",
        "    history_valid.append(cross_entropy_jit(x_valid, y_valid, weights, bias))\n",
        "\n",
        "\n",
        "  for epoch in tqdm(range(num_epochs)):  \n",
        "    # take indexes of the batch\n",
        "    idxs = np.random.choice(x_train.shape[0], batch_size) \n",
        "\n",
        "    # compute gradient\n",
        "    grads = grad_jit(x_train[idxs,:], y_train[idxs,:], weights, bias)\n",
        "    \n",
        "    # compute learning rate value\n",
        "    learning_rate = max(learning_rate_min, learning_rate_max * (1 - epoch/learning_rate_decay))\n",
        "    \n",
        "    # update weights and bias\n",
        "    weights -= learning_rate * grads[0]\n",
        "    bias -= learning_rate * grads[1]\n",
        "\n",
        "    # compute value of the cost function on entire dataset, it is done periodically because\n",
        "    # if the dataset is big, this operation slows down too much the function\n",
        "    if epoch % 10 == 0:\n",
        "      history.append(cross_entropy_jit(x_train, y_train, weights, bias)) \n",
        "      if validation:\n",
        "        history_valid.append(cross_entropy_jit(x_valid, y_valid, weights, bias))\n",
        "\n",
        "  #fig, axs = plt.subplots(1,1,figsize=(15,10))\n",
        "  #print('train loss: %1.3e' % history[-1]) #confrontare con altre loss, es mse o accuracy\n",
        "  #axs.plot(history, label='train')\n",
        "  #if validation:\n",
        "  #  print('validation loss: %1.3e' % history_valid[-1]) #confrontare con altre loss, es mse o accuracy\n",
        "  #  axs.plot(history_valid, label='validation')\n",
        "  #axs.legend()\n",
        "\n",
        "  return weights, bias, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REyZTIE9KquG"
      },
      "outputs": [],
      "source": [
        "def predict_LR(x, weights, bias):\n",
        "  '''\n",
        "  Predicts output values using logistic regression\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x : ndarray\n",
        "    samples\n",
        "  weights : ndarray\n",
        "    weights of the logistic regression model\n",
        "  bias : float\n",
        "    bias of the logistic regression model\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  predictions : ndarray\n",
        "    array with values of the samples predicted with the given model\n",
        "  '''\n",
        "  return 1. / (1. + np.exp(-(bias + x @ weights)))\n",
        "\n",
        "def predict_label_LR(x, weights, bias):\n",
        "  '''\n",
        "  Predicts labels using logistic regression\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x : ndarray\n",
        "    samples\n",
        "  weights : ndarray\n",
        "    weights of the logistic regression model\n",
        "  bias : float\n",
        "    bias of the logistic regression model\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  predictions : ndarray\n",
        "    array with labels of the samples predicted with the given model\n",
        "  '''\n",
        "\n",
        "  # predict output of samples with the model\n",
        "  y_pred = predict_LR(x, weights, bias)\n",
        "\n",
        "  # link the predicted output to the label\n",
        "  predictions = np.array([0. if pred < 0.5 else 1. for pred in y_pred])[:, None] # by setting the threshold at 0.4 better results\n",
        "  return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLyQMU-KzPAZ"
      },
      "source": [
        "## Testing LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFbujhvbzSPj"
      },
      "outputs": [],
      "source": [
        "# test LR on a single dataset\n",
        "\n",
        "# set up the datasets\n",
        "data_np = data_5.to_numpy()\n",
        "data_normalized_np = min_max(data_np)\n",
        "x_train, y_train, _, _, x_test, y_test = train_split(data_normalized_np, percentage_train=0.7, percentage_validation=0.0)\n",
        "x_train, y_train = undersample(x_train, y_train, 50.)\n",
        "\n",
        "# train model\n",
        "weights, bias, history = SGD(x_train, y_train) \n",
        "\n",
        "# plot history of the values of loss function\n",
        "fig, axs = plt.subplots(1,1,figsize = (15,10)) \n",
        "axs.plot(history)\n",
        "axs.set_title('train loss: %1.3e' % history[-1])\n",
        "\n",
        "# get predictions with the trained model\n",
        "predictions = predict_label_LR(x_test, weights, bias) \n",
        "\n",
        "# compute metrics\n",
        "metrics(predictions, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing LR on all datasets \n",
        "datasets = [data_1, data_2, data_3, data_4, data_5, data_6, data_7]\n",
        "fig, axs = plt.subplots(8,1,figsize = (15,80)) \n",
        "weights_list = []\n",
        "bias_list = []\n",
        "\n",
        "# train the classifiers\n",
        "for i, dataset in enumerate(datasets):\n",
        "  # set up the datasets\n",
        "  dataset_np = dataset.to_numpy()\n",
        "  dataset_normalized_np = min_max(dataset_np)\n",
        "  x_train, y_train, _, _, x_test, y_test = train_split(dataset_normalized_np, percentage_train=0.7, percentage_validation=0.0)\n",
        "  x_train, y_train = undersample(x_train, y_train, 20.)\n",
        "\n",
        "  # train model\n",
        "  weights, bias, history = SGD(x_train, y_train, batch_size=4, num_epochs=25000) \n",
        "\n",
        "  # save the model\n",
        "  weights_list.append(weights)\n",
        "  bias_list.append(bias)\n",
        "\n",
        "  # plot training history of the values of loss function\n",
        "  axs[i].plot(history)\n",
        "  axs[i].set_title('train loss for dataset %d: %1.3e' % (i+1, history[-1]))\n",
        "\n",
        "  # get predictions with the trained model on train set\n",
        "  predictions = predict_label_LR(x_train, weights, bias) \n",
        "  \n",
        "  # compute metrics on train set\n",
        "  print(\"Metrics for dataset %d\" %(i+1))\n",
        "  print(\"Train set\")\n",
        "  metrics(predictions, y_train) \n",
        "\n",
        "  # get predictions with the trained model on test set\n",
        "  predictions = predict_label_LR(x_test, weights, bias) \n",
        "\n",
        "  # compute metrics on test set\n",
        "  print(\"Test set\")\n",
        "  metrics(predictions, y_test) \n",
        "  #works well also with 30 undersample and batch_size=8\n",
        "\n",
        "  # compute ROC and AUC\n",
        "  y_pred = predict_LR(x_test, weights, bias)\n",
        "  fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "\n",
        "  print(\"AUC: %.2f\" % roc_auc)\n",
        "  # Plot the ROC curve\n",
        "  axs[7].plot(fpr, tpr, label='AUC dataset %d = %0.2f' % (i+1, roc_auc))\n",
        "  \n",
        "axs[7].set_title('ROC Curve')\n",
        "axs[7].legend(loc='lower right')\n",
        "axs[7].set_xlabel('False Positive Rate')\n",
        "axs[7].set_ylabel('True Positive Rate')"
      ],
      "metadata": {
        "id": "lmz_RM5-XZK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Decision tree"
      ],
      "metadata": {
        "id": "wqDFCfjD7jDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node: \n",
        "  '''\n",
        "  Class representing a node in a DecisionTree\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  feature : int\n",
        "    feature associated to the node\n",
        "  threshold : float\n",
        "    threshold associated to the feature\n",
        "  left : Node\n",
        "    left child of the node\n",
        "  right : Node\n",
        "    right child of the node\n",
        "  value : float\n",
        "    label associated to the node, if leaf\n",
        "\n",
        "  Methods\n",
        "  -------\n",
        "  is_leaf : tell if a Node is a leaf or not\n",
        "  '''\n",
        " \n",
        "  def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    feature : int\n",
        "      feature associated to the node (default is None)\n",
        "    threshold : float\n",
        "      threshold associated to the feature (default is None)\n",
        "    left : Node\n",
        "      left child of the node (default is None)\n",
        "    right : Node\n",
        "      right child of the node (default is None)\n",
        "    value : float\n",
        "      label associated to the node, if leaf (default is None)\n",
        "    '''\n",
        "\n",
        "    self.feature = feature\n",
        "    self.threshold = threshold\n",
        "    self.left = left\n",
        "    self.right = right\n",
        "    self.value = value\n",
        "\n",
        "  def is_leaf(self):\n",
        "    '''\n",
        "    Tells if the node is a leaf\n",
        "    '''\n",
        "\n",
        "    return self.value is not None\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "  '''\n",
        "  Class representing a decision tree\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  max_depth : int\n",
        "    max depth of a tree\n",
        "  n_features : int\n",
        "    number of features to evaluate while branching to build a tree\n",
        "  max_number_thresholds : int\n",
        "    max number of thresholds to evaluate while branching to build a tree\n",
        "  min_samples : int\n",
        "    min number of samples in a node to branch when building a tree\n",
        "  root : Node\n",
        "    root of the tree\n",
        "  mode : str\n",
        "      mode to compute the information gain, it can be 'gini' or 'entropy'\n",
        "  \n",
        "  Methods\n",
        "  -------\n",
        "  train : to train a decision tree model\n",
        "  predict : to make predictions using the built model\n",
        "  '''\n",
        "\n",
        "  def __init__(self, max_depth=10, n_features=None, max_number_thresholds=np.inf, min_samples=2, mode='gini'):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    max_depth : int\n",
        "      max depth of a tree (default is 10)\n",
        "    n_features : int\n",
        "      number of features to evaluate while branching to build a tree (default is None)\n",
        "    max_number_thresholds : int\n",
        "      max number of thresholds to evaluate while branching to build a tree (default is np.inf)\n",
        "    min_samples : int\n",
        "      min number of samples in a node to branch when building a tree (default is 2)\n",
        "    mode : str\n",
        "      mode to compute the information gain, it can be 'gini' or 'entropy' (default is 'gini')\n",
        "    '''\n",
        "\n",
        "    self.max_depth = max_depth \n",
        "    self.n_features = n_features \n",
        "    self.max_number_thresholds = max_number_thresholds \n",
        "    self.min_samples = min_samples  \n",
        "    self.root = None\n",
        "    self.mode = mode\n",
        "\n",
        "  def train(self, x, y):\n",
        "    '''\n",
        "    Builds the tree\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray\n",
        "      train dataset with samples on rows \n",
        "    y : ndarray\n",
        "      labels associated to the dataset\n",
        "    '''\n",
        "\n",
        "    if self.n_features is None or self.n_features > x.shape[1]:\n",
        "      self.n_features = x.shape[1] \n",
        "    self.root = self.build_tree(x, y)\n",
        "      \n",
        "  def build_tree(self, x, y, depth=0):\n",
        "    '''\n",
        "    Builds the tree recursively and returns the root\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray\n",
        "      train dataset with samples on rows\n",
        "    y : ndarray\n",
        "      labels associated to the dataset\n",
        "    depth : int\n",
        "      current depth of the tree\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Node\n",
        "      the root of the built tree\n",
        "    '''\n",
        "\n",
        "    num_samples, num_features = x.shape\n",
        "    num_values = len(np.unique(y)) # get number of different values in y \n",
        "\n",
        "    # check the stopping criteria, if so create a leaf node\n",
        "    if depth >= self.max_depth or num_samples < self.min_samples or num_values <= 1:\n",
        "      return Node(value = self.get_value(y))\n",
        "\n",
        "    # randomly select the features to evaluate\n",
        "    features = np.random.choice(num_features, self.n_features, replace=False)\n",
        "\n",
        "    # to select the best branch\n",
        "    max_gain = -1\n",
        "    branch_threshold= None\n",
        "    branch_feature = None\n",
        "\n",
        "    #print('split')\n",
        "    # evaluate all selected features\n",
        "    for feature in features:\n",
        "      # randomly select the thresholds to evaluate\n",
        "      thresholds = np.random.choice(np.unique(x[:, feature]), \n",
        "                                    min(self.max_number_thresholds, len(np.unique(x[:, feature]))), replace=False)\n",
        "\n",
        "      # evaluate all selected thresholds\n",
        "      for threshold in thresholds:\n",
        "        # compute the information gain\n",
        "        gain = self.gain(x[:, feature], y, threshold)\n",
        "\n",
        "        # check if it is the best gain\n",
        "        if gain > max_gain:\n",
        "          max_gain = gain\n",
        "          branch_threshold = threshold\n",
        "          branch_feature = feature\n",
        "\n",
        "    # build left branch\n",
        "    left_idxs = x[:, branch_feature] <= branch_threshold\n",
        "    left = self.build_tree(x[left_idxs, :], y[left_idxs, :], depth+1)\n",
        "\n",
        "    # build right branch\n",
        "    right_idxs = x[:, branch_feature] > branch_threshold\n",
        "    right = self.build_tree(x[right_idxs, :], y[right_idxs, :], depth+1)\n",
        "\n",
        "    return Node(branch_feature, branch_threshold, left, right)\n",
        "        \n",
        "  def gain(self, x, y, threshold):\n",
        "    '''\n",
        "    Computes the information gain using gini or entropy\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray\n",
        "      train dataset with samples on rows\n",
        "    y : ndarray\n",
        "      labels associated to the dataset\n",
        "    threshold : float\n",
        "      threshold to divide the samples\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "      the value of the information gain \n",
        "    '''\n",
        "\n",
        "    # take indexes of left and right children\n",
        "    left_idxs = x <= threshold\n",
        "    right_idxs = x > threshold\n",
        "\n",
        "    # if one of the children is empty nothing is changing, so the gain is 0\n",
        "    if np.sum(left_idxs) == 0 or np.sum(right_idxs) == 0:\n",
        "      return 0.\n",
        "\n",
        "    # compute the weights of the children\n",
        "    weight_left = np.sum(left_idxs) / len(y)\n",
        "    weight_right = np.sum(right_idxs) / len(y)\n",
        "\n",
        "    # compute information gain based on the mode\n",
        "    if self.mode == 'gini':\n",
        "      return self.gini(y) - (weight_left * self.gini(y[left_idxs, :]) + weight_right * self.gini(y[right_idxs, :]))\n",
        "    elif self.mode == 'entropy':\n",
        "      return self.entropy(y) - (weight_left * self.entropy(y[left_idxs, :]) + weight_right * self.entropy(y[right_idxs, :]))\n",
        "\n",
        "  def gini(self, y):\n",
        "    '''\n",
        "    Computes the gini index of a vector of labels\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y : ndarray\n",
        "      vector of labels containing values 0. or 1.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "      the value of the gini index\n",
        "    '''\n",
        "\n",
        "    # compute probability of having 1. and 0.\n",
        "    p_ones = np.sum(y) / len(y)\n",
        "    p_zeros = 1. - p_ones\n",
        "\n",
        "    # compute gini index\n",
        "    return 1. - (p_zeros**2 + p_ones**2)\n",
        "\n",
        "  def entropy(self, y): # using gini would not require the logarithm, more efficient\n",
        "    '''\n",
        "    Computes the entropy of a vector of labels\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y : ndarray\n",
        "      vector of labels containing values 0. or 1. \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "      the entropy of the vector\n",
        "    '''\n",
        "\n",
        "    # compute probability of having 1. and 0.\n",
        "    p_ones = np.sum(y) / len(y)\n",
        "    p_zeros = 1. - p_ones\n",
        "\n",
        "    # compute entropy\n",
        "    # to avoid computing logarithm of 0.\n",
        "    if p_ones == 0. or  p_zeros == 0.:\n",
        "      return 0\n",
        "    return -(p_zeros * np.log(p_zeros) + p_ones * np.log(p_ones))\n",
        "\n",
        "  def get_value(self, y):\n",
        "    '''\n",
        "    Finds the most common label in a vector\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y : ndarray\n",
        "      vector of labels containing values 0. or 1.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "      the value of the most common label\n",
        "    '''\n",
        "\n",
        "    ones = np.sum(y)\n",
        "    zeros = len(y) - ones\n",
        "    return 1. if ones >= zeros else 0.\n",
        "\n",
        "  def predict(self, x):\n",
        "    '''\n",
        "    Returns the predictions on the input dataset\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray\n",
        "      test dataset with samples on rows\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ndarray\n",
        "      predicted labels\n",
        "    '''\n",
        "    \n",
        "    # compute prediction for each sample of the input\n",
        "    return np.array([self.get_prediction(sample, self.root) for sample in x])[:,None] # return a column vector\n",
        "\n",
        "  def get_prediction(self, x, node):\n",
        "    '''\n",
        "    Recursively traverse the tree to make a prediction\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray\n",
        "      sample to predict\n",
        "    node : Node\n",
        "      current node of the tree\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "      predicted label\n",
        "    '''\n",
        "\n",
        "    # if the node is a leaf return the prediction\n",
        "    if node.is_leaf():\n",
        "      return node.value\n",
        "    \n",
        "    # if the value of the sample corresponding to the feature of the node \n",
        "    # is less than the threshold of the node go left, otherwise go right\n",
        "    if x[node.feature] <= node.threshold:\n",
        "      return self.get_prediction(x, node.left)\n",
        "    else:\n",
        "      return self.get_prediction(x, node.right)"
      ],
      "metadata": {
        "id": "Unqbtpmi7m4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing DT"
      ],
      "metadata": {
        "id": "hkXbr1i1snCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test DT on a single dataset\n",
        "\n",
        "# set up the datasets\n",
        "data_np = data_4.to_numpy()\n",
        "data_normalized_np = min_max(data_np)\n",
        "x_train, y_train, _, _, x_test, y_test = train_split(data_normalized_np, percentage_train=0.7, percentage_validation=0.0)\n",
        "\n",
        "# train model\n",
        "classifier = DecisionTree(max_depth=7, n_features=4, max_number_thresholds=100, min_samples=5) # higher max_thresh performs better but takes more time to train\n",
        "classifier.train(x_train, y_train)\n",
        "\n",
        "# get predictions with the trained model\n",
        "predictions = classifier.predict(x_test)\n",
        "\n",
        "# compute metrics\n",
        "metrics(predictions, y_test) \n",
        "\n",
        "# pruning, or require a minimum number of samples per leaf per non rischiare overfitting"
      ],
      "metadata": {
        "id": "a_DLCjlL76JN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing DT on all datasets \n",
        "datasets = [data_1, data_2, data_3, data_4, data_5, data_6, data_7]\n",
        "classifiers = []\n",
        "\n",
        "for i, dataset in enumerate(datasets):\n",
        "  # set up the datasets\n",
        "  dataset_np = dataset.to_numpy()\n",
        "  dataset_normalized_np = min_max(dataset_np)\n",
        "  x_train, y_train, _, _, x_test, y_test = train_split(dataset_normalized_np, percentage_train=0.7, percentage_validation=0.0)\n",
        "  x_train, y_train = undersample(x_train, y_train, 200.)\n",
        "\n",
        "  # train model\n",
        "  classifier = DecisionTree(max_depth=7, max_number_thresholds=50) # higher max_thresh performs better but takes more time to train\n",
        "  classifier.train(x_train, y_train)\n",
        "\n",
        "  # save the model\n",
        "  classifiers.append(classifier)\n",
        "\n",
        "  # get predictions with the trained model on train set\n",
        "  predictions = classifier.predict(x_train)\n",
        "\n",
        "  # compute metrics on train test\n",
        "  print(\"\\nMetrics for dataset %d\" %(i+1))\n",
        "  print(\"Train set\")\n",
        "  metrics(predictions, y_train) \n",
        "\n",
        "  # get predictions with the trained model on test set\n",
        "  predictions = classifier.predict(x_test)\n",
        "\n",
        "  # compute metrics on test set\n",
        "  print(\"Test set\")\n",
        "  metrics(predictions, y_test) \n",
        "\n",
        "  fpr, tpr, _ = roc_curve(y_test, predictions)\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "\n",
        "  print(\"AUC: %.2f\" % roc_auc)\n",
        "  # Plot the ROC curve\n",
        "  plt.plot(fpr, tpr, label='AUC dataset %d = %0.2f' % (i+1, roc_auc))\n",
        "  \n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NHcIJqEfdm9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random forest"
      ],
      "metadata": {
        "id": "dHzCLi29W6Sn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomForest:\n",
        "  '''\n",
        "  Class representing a random forest\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  n_trees : int \n",
        "    number of trees in the forest\n",
        "  max_depth : int \n",
        "    max depth of a tree\n",
        "  n_features : int\n",
        "    number of features to evaluate while branching to build a tree\n",
        "  max_number_thresholds : int\n",
        "    max number of thresholds to evaluate while branching to build a tree\n",
        "  min_samples : int\n",
        "    min number of samples in a node to branch when building a tree\n",
        "  trees : list\n",
        "    list with all the trees\n",
        "\n",
        "  Methods\n",
        "  -------\n",
        "  train : to train a random forest model\n",
        "  predict : to make predictions using the built model\n",
        "  '''\n",
        "\n",
        "  def __init__(self, n_trees=50, max_depth=10, n_features=None, max_number_thresholds=np.inf, min_samples=2):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_trees : int\n",
        "      number of trees in the forest (default is 50)\n",
        "    max_depth : int\n",
        "      max depth of a tree (default is 10)\n",
        "    n_features : int\n",
        "      number of features to evaluate while branching to build a tree (default is None)\n",
        "    max_number_thresholds : int\n",
        "      max number of thresholds to evaluate while branching to build a tree (default is np.inf)\n",
        "    min_samples : int\n",
        "      min number of samples in a node to branch when building a tree (default is 2)\n",
        "    '''\n",
        "\n",
        "    self.n_trees = n_trees\n",
        "    self.max_depth = max_depth \n",
        "    self.n_features = n_features \n",
        "    self.max_number_thresholds = max_number_thresholds \n",
        "    self.min_samples = min_samples\n",
        "    self.trees = []\n",
        "  \n",
        "  def train(self, x, y):\n",
        "    '''\n",
        "    Builds the forest\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray\n",
        "      train dataset with samples on rows \n",
        "    y : ndarray\n",
        "      labels associated to the dataset\n",
        "    ''' \n",
        "\n",
        "    for i in tqdm(range(self.n_trees)):\n",
        "      # create bootstrapped dataset\n",
        "      n_samples = x.shape[0]\n",
        "      idxs = np.random.choice(n_samples, n_samples, replace=True) # with replacement\n",
        "      \n",
        "      # train a tree\n",
        "      tree = DecisionTree(self.max_depth, self.n_features, self.max_number_thresholds, self.min_samples)\n",
        "      tree.train(x[idxs, :], y[idxs, :])\n",
        "      self.trees.append(tree)\n",
        "\n",
        "  # obviously this require a hyperparameter more to tune\n",
        "  def validate(self, x, y, parameter):\n",
        "    votes = np.array([tree.predict(x) for tree in self.trees])\n",
        "    votes = votes.reshape((votes.shape[1], votes.shape[0]))\n",
        "    check = votes == y\n",
        "    check = np.sum(check, axis=0) / len(y)\n",
        "    print(check)\n",
        "    survive = check >= parameter\n",
        "    print('survived %d trees' %(np.sum(survive)))\n",
        "    for i, e in reversed(list(enumerate(survive))):\n",
        "      if e == False:\n",
        "        del self.trees[i]\n",
        "\n",
        "  def predict(self, x):\n",
        "    '''\n",
        "    Returns the predictions by majority vote\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray\n",
        "      test dataset with samples on rows\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    majorities : ndarray\n",
        "      predicted labels\n",
        "    '''\n",
        "\n",
        "    # make predictions for each tree \n",
        "    votes = np.array([tree.predict(x) for tree in self.trees])\n",
        "    \n",
        "    # compute majority \n",
        "    majorities = np.mean(votes, axis=0)\n",
        "    \n",
        "    # associate the labels\n",
        "    majorities[majorities >= 0.5] = 1.\n",
        "    majorities[majorities < 0.5] = 0.\n",
        "    return majorities, votes"
      ],
      "metadata": {
        "id": "Xz467gI3W-GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing RF"
      ],
      "metadata": {
        "id": "5zW--vvdXnUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test RF on a single dataset\n",
        "\n",
        "# set up the datasets\n",
        "data_np = data_5.to_numpy()\n",
        "data_normalized_np = min_max(data_np)\n",
        "x_train, y_train, x_valid, y_valid, x_test, y_test = train_split(data_normalized_np, percentage_train=0.6, percentage_validation=0.2)\n",
        "x_train, y_train = undersample(x_train, y_train, 50.)\n",
        "\n",
        "# train model\n",
        "classifier = RandomForest(n_trees=50, max_depth=5, n_features=3, max_number_thresholds=50) \n",
        "classifier.train(x_train, y_train)\n",
        "\n",
        "#classifier.validate(x_valid, y_valid, 0.996)\n",
        "\n",
        "# get predictions with the trained model\n",
        "predictions,_ = classifier.predict(x_test)\n",
        "\n",
        "# compute metrics\n",
        "metrics(predictions, y_test) \n"
      ],
      "metadata": {
        "id": "ibr4snEMXOCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing DT on all datasets \n",
        "datasets = [data_1, data_2, data_3, data_4, data_5, data_6, data_7]\n",
        "\n",
        "for i, dataset in enumerate(datasets):\n",
        "  # set up the datasets\n",
        "  dataset_np = dataset.to_numpy()\n",
        "  dataset_normalized_np = min_max(dataset_np)\n",
        "  x_train, y_train, _, _, x_test, y_test = train_split(dataset_normalized_np, percentage_train=0.6, percentage_validation=0.0)\n",
        "  x_train, y_train = undersample(x_train, y_train, 200.)\n",
        "\n",
        "  # train model\n",
        "  classifier = RandomForest(n_trees=20, max_depth=7, n_features=4, max_number_thresholds=50)  # better with 20 trees instead of 50\n",
        "  classifier.train(x_train, y_train)\n",
        "\n",
        "  # get predictions with the trained model on train set\n",
        "  predictions, _ = classifier.predict(x_train)\n",
        "\n",
        "  # compute metrics on train test\n",
        "  print(\"\\nMetrics for dataset %d\" %(i+1))\n",
        "  print(\"Train set\")\n",
        "  metrics(predictions, y_train) \n",
        "\n",
        "  # get predictions with the trained model\n",
        "  predictions, votes = classifier.predict(x_test)\n",
        "\n",
        "  # get predictions with the trained model on test set\n",
        "  print(\"Test set\")\n",
        "  metrics(predictions, y_test) \n",
        "  \n",
        "  # compute majority \n",
        "  majorities = np.mean(votes, axis=0)\n",
        "  fpr, tpr, _ = roc_curve(y_test, majorities)\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "\n",
        "  print(\"AUC: %.2f\" % roc_auc)\n",
        "  # Plot the ROC curve\n",
        "  plt.plot(fpr, tpr, label='AUC dataset %d = %0.2f' % (i+1, roc_auc))\n",
        "  \n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TWl-JuujYGC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Artificial Neural Network Function Definition"
      ],
      "metadata": {
        "id": "rTQWdgH-7UVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import jax.numpy as jnp\n",
        "import jax"
      ],
      "metadata": {
        "id": "Pqz4wHlJ7ght"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(layers_size):\n",
        "  '''\n",
        "  Returns the parameters of the artificial neural network given the number of\n",
        "  neurons of its layers, namely it returns the matrices of weights and the bias\n",
        "  vector for each layer \n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  layers_size: list\n",
        "    ordered sizes of the layers of the artificial neural network it is required\n",
        "    to create\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  list\n",
        "    parameters of the artificial neural network\n",
        "  '''\n",
        "\n",
        "  np.random.seed(0)\n",
        "  parameters = list()\n",
        "\n",
        "  for i in range(len(layers_size) - 1):\n",
        "    W = np.random.randn(layers_size[i+1], layers_size[i])\n",
        "    b = np.zeros((layers_size[i+1], 1))\n",
        "    parameters.append(W)\n",
        "    parameters.append(b)\n",
        "  \n",
        "  return parameters"
      ],
      "metadata": {
        "id": "0t7cHl3O7lG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ANN(x, parameters, feature_on_columns=True):\n",
        "  '''\n",
        "  Commputes the value of the output of the artificial neural network identified\n",
        "  by its parameters given an input\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x: ndarray\n",
        "    input of the artificial neural network\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "  feature_on_columns: bool\n",
        "    True, if the features are on the columns of the input, False, if the\n",
        "    features are on the rows of the input\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  ndarray\n",
        "    output value of the artificial neural network\n",
        "  '''\n",
        "\n",
        "  if len(parameters)%2!=0:\n",
        "    raise Exception(\"The input parameters must be in even number\")\n",
        "  \n",
        "  if feature_on_columns:\n",
        "    layer = x.T\n",
        "  \n",
        "  num_layers = int(len(parameters)/2)+1\n",
        "  weights = parameters[0::2]\n",
        "  biases = parameters[1::2]\n",
        "\n",
        "  for i in range(num_layers-1):\n",
        "    layer = weights[i] @ layer - biases[i]\n",
        "    \n",
        "    # Activation function is applied to all the layers since the output is \n",
        "    # needed to be between 0 and 1\n",
        "    layer = jax.nn.sigmoid(layer)\n",
        "\n",
        "  if feature_on_columns:\n",
        "    layer = layer.T\n",
        "  \n",
        "  return layer"
      ],
      "metadata": {
        "id": "A8ufUeKQ7pm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(x, y, parameters):\n",
        "  '''\n",
        "  Computes the cross entropy cost function in the case of an artificial neural\n",
        "  network for a binary classification problem\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x: ndarray\n",
        "    input of the artificial neural network\n",
        "  y: ndarray\n",
        "    correct value of the output\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  float\n",
        "    cross entropy between the predictions of the artificial neural network\n",
        "    and the correct value\n",
        "  '''\n",
        "  y_pred = ANN(x, parameters)\n",
        "  return -jnp.mean(2 * y * jnp.log(y_pred) + 0.5 * (1 - y) * jnp.log(1 - y_pred))\n",
        "\n",
        "def cross_entropy_general(x, y, parameters):\n",
        "  '''\n",
        "  Computes the cross entropy cost function in the case of an artificial neural\n",
        "  network for a general classification problem, namely with an artificial neural\n",
        "  network with the number of outputs equals to the number of classes\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x: ndarray\n",
        "    input of the artificial neural network\n",
        "  y: ndarray\n",
        "    correct value of the output, one-hot representation\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  float\n",
        "    cross entropy between the predictions of the artificial neural network\n",
        "    and the correct value\n",
        "  '''\n",
        "  y_pred = ANN(x, parameters)\n",
        "  return -jnp.mean(jnp.sum(y * jnp.log(y_pred), axis=1))\n",
        "\n",
        "def MSE(x, y, parameters):\n",
        "  '''\n",
        "  Computes the mean squared error in the case of an artificial neural network\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  x: ndarray\n",
        "    input of the artificial neural network\n",
        "  y: ndarray\n",
        "    correct value of the output\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  float\n",
        "    mean square error between the predictions of the artificial neural network\n",
        "    and the correct value\n",
        "  '''\n",
        "\n",
        "  y_pred = ANN(x, parameters)\n",
        "  return jnp.mean(jnp.square(y - y_pred))\n",
        "\n",
        "def accuracy(x, y, parameters, threshold=0.4):\n",
        "  '''\n",
        "  Compute the accuracy of the prediction in the case of an artificial neural\n",
        "  network for a binary classification problem\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x: ndarray\n",
        "    input of the artificial neural network\n",
        "  y: ndarray\n",
        "    correct value of the output\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "  threshold: float\n",
        "    output value of the artificial neural network over which a sample is\n",
        "    considered to belong to the positive class\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  float\n",
        "    accuracy of the artificial neural network, namely the number of samples\n",
        "    correctly classified divided by the total number of samples\n",
        "  '''\n",
        "  y_pred = ANN(x, parameters)\n",
        "  labels_pred = y_pred > threshold\n",
        "  return jnp.mean(y == labels_pred)\n",
        "\n",
        "# Computation of the gradient and JIT compilation of the loss functions\n",
        "\n",
        "# Gradient of the cross entropy for binary classification problems\n",
        "grad_jit = jax.jit(jax.grad(cross_entropy, argnums = 2))\n",
        "\n",
        "# Gradient of the cross entropy for general classification problems\n",
        "#grad_jit = jax.jit(jax.grad(cross_entropy_general, argnums = 2))\n",
        "\n",
        "# Gradient of MSE\n",
        "#grad_jit = jax.jit(jax.grad(MSE, argnums = 2))\n",
        "\n",
        "# JIT compiled loss functions\n",
        "cross_entropy_jit = jax.jit(cross_entropy)\n",
        "cross_entropy_general_jit = jax.jit(cross_entropy)\n",
        "MSE_jit = jax.jit(MSE)\n",
        "accuracy_jit = jax.jit(accuracy)"
      ],
      "metadata": {
        "id": "qzUCZimF7sSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dump(x_train, y_train, parameters, histories, x_valid=None, y_valid=None):\n",
        "  '''\n",
        "  Updates the history of the values of the loss functions for the training set\n",
        "  and for the validation set\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x_train: ndarray\n",
        "    input of the artificial neural network used for the training phase\n",
        "  y_train: ndarray\n",
        "    correct value of the output of the training set\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "  histories: dict\n",
        "    current history of the loss functions to which a new measurement has to be\n",
        "    added\n",
        "  x_valid: ndarray\n",
        "    input of the artificial neural network used for validating the model\n",
        "  y_valid: ndarray\n",
        "    correct value of the output of the validation set\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  dict\n",
        "    histories of the loss functions\n",
        "  '''\n",
        "\n",
        "  if histories is None or len(histories) == 0:\n",
        "    histories = {'Xen_train': [], 'MSE_train': [], 'acc_train': []}\n",
        "  if x_valid is not None and len(x_valid) != 0 and y_valid is not None and len(y_valid) != 0 and 'Xen_valid' not in histories.keys():\n",
        "    histories['Xen_valid'] = []\n",
        "    histories['MSE_valid'] = []\n",
        "    histories['acc_valid'] = []\n",
        "  elif x_valid is not None and len(x_valid) != 0 and y_valid is not None and len(y_valid) != 0:\n",
        "    histories['Xen_valid'].append(cross_entropy_jit(x_valid, y_valid, parameters))\n",
        "    histories['MSE_valid'].append(MSE_jit(x_valid, y_valid, parameters))\n",
        "    histories['acc_valid'].append(accuracy_jit(x_valid, y_valid, parameters))\n",
        "  histories['Xen_train'].append(cross_entropy_jit(x_train, y_train, parameters))\n",
        "  histories['MSE_train'].append(MSE_jit(x_train, y_train, parameters))\n",
        "  histories['acc_train'].append(accuracy_jit(x_train, y_train, parameters))\n",
        "  return histories"
      ],
      "metadata": {
        "id": "WEF2x3bqEYgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(histories):\n",
        "  '''\n",
        "  Plots the histories of the losses during execution\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  histories: dict\n",
        "    values of the loss functions over time to be plotted\n",
        "  '''\n",
        "  \n",
        "  fig, axs = plt.subplots(len(histories), 1, figsize=(8, 8*len(histories)))\n",
        "  axs = axs.flatten()\n",
        "  for i, key in enumerate(histories):\n",
        "    axs[i].plot(histories[key])\n",
        "    axs[i].set_title(list(histories.keys())[i])"
      ],
      "metadata": {
        "id": "uY-Nzy-5FMy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SGD(x_train, y_train, x_valid, y_valid, parameters, learning_rate_min=1e-4,\\\n",
        "        learning_rate_max=1e-1, learning_rate_decay=1000, num_epochs=1000,\\\n",
        "        batch_size=64):\n",
        "  \"\"\"\n",
        "  Stochastic gradient descent method with mini-batch and learning rate decay\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x_train: ndarray\n",
        "    training set of the dataset to fit\n",
        "  y_train: ndarray\n",
        "    labels of the samples belonging to the training set\n",
        "  x_valid: ndarray\n",
        "    validation set of the dataset to fit\n",
        "  y_valid: ndarray\n",
        "    labels of the samples belonging to the validation set\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "  learning_rate_min: float\n",
        "    minimum learning rate used in the training phase\n",
        "  learning_rate_max: float\n",
        "    maximum learning rate used in the training phase\n",
        "  learning_rate_decay: float\n",
        "    learning rate decay used in the training phase\n",
        "  num_epochs: int\n",
        "    number of epochs to perform\n",
        "  batch_size: int\n",
        "    size of the batches to be used for computing the gradient\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  parameters: list\n",
        "    trained parameters of the artificial neural network, namely weights and\n",
        "    biases optimized for fitting the training set\n",
        "  histories: dict\n",
        "    values of the loss functions over time\n",
        "  \"\"\"\n",
        "  num_samples = x_train.shape[0]\n",
        "\n",
        "  histories = dump(x_train, y_train, parameters, None, x_valid=x_valid, y_valid=y_valid)\n",
        "\n",
        "  for epoch in tqdm(range(num_epochs)):\n",
        "    learning_rate = max(learning_rate_min, learning_rate_max * (1 - epoch/learning_rate_decay))\n",
        "\n",
        "    idxs = np.random.choice(num_samples, batch_size)\n",
        "    grads = grad_jit(x_train[idxs,:], y_train[idxs,:], parameters)\n",
        "    for i in range(len(parameters)):\n",
        "      parameters[i] -= learning_rate * grads[i]\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "      dump(x_train, y_train, parameters, histories, x_valid=x_valid, y_valid=y_valid)\n",
        "\n",
        "  return parameters, histories"
      ],
      "metadata": {
        "id": "RTLoCYThppg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NAG(x_train, y_train, x_valid, y_valid, parameters, learning_rate_min=1e-5,\\\n",
        "        learning_rate_max=1e-1, learning_rate_decay=1500., num_epochs=3000,\\\n",
        "        batch_size=128, alpha=0.9):\n",
        "  \"\"\"\n",
        "  Implements the Nesterov accelleration method with mini-batch and learning rate\n",
        "  decay\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x_train: ndarray\n",
        "    training set of the dataset to fit\n",
        "  y_train: ndarray\n",
        "    labels of the samples belonging to the training set\n",
        "  x_valid: ndarray\n",
        "    validation set of the dataset to fit\n",
        "  y_valid: ndarray\n",
        "    labels of the samples belonging to the validation set\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "  learning_rate_min: float\n",
        "    minimum learning rate used in the training phase\n",
        "  learning_rate_max: float\n",
        "    maximum learning rate used in the training phase\n",
        "  learning_rate_decay: float\n",
        "    learning rate decay used in the training phase\n",
        "  num_epochs: int\n",
        "    number of epochs to perform\n",
        "  batch_size: int\n",
        "    size of the batches to be used for computing the gradient\n",
        "  alpha: float\n",
        "    weight of the velocity vector at each iteration\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  parameters: list\n",
        "    trained parameters of the artificial neural network, namely weights and\n",
        "    biases optimized for fitting the training set\n",
        "  histories: dict\n",
        "    values of the loss functions over time\n",
        "  \"\"\"\n",
        "  \n",
        "  num_samples = x_train.shape[0]\n",
        "  velocity = [0.0 for i in range(len(parameters))]\n",
        "  grad_args = [0.0 for i in range(len(parameters))]\n",
        "\n",
        "  histories = dump(x_train, y_train, parameters, None, x_valid=x_valid, y_valid=y_valid)\n",
        "\n",
        "  for epoch in tqdm(range(num_epochs)):\n",
        "    learning_rate = max(learning_rate_min, learning_rate_max * (1 - epoch/learning_rate_decay))\n",
        "\n",
        "    for i in range(len(parameters)):\n",
        "      grad_args[i] = parameters[i] - alpha * velocity[i]\n",
        "    idxs = np.random.choice(num_samples, batch_size)\n",
        "    grads = grad_jit(x_train[idxs,:], y_train[idxs,:], grad_args)\n",
        "    for i in range(len(parameters)):\n",
        "      velocity[i] = alpha * velocity[i] + learning_rate * grads[i]\n",
        "      parameters[i] -= velocity[i]\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "      dump(x_train, y_train, parameters, histories, x_valid=x_valid, y_valid=y_valid)\n",
        "\n",
        "  return parameters, histories"
      ],
      "metadata": {
        "id": "GRo18nNu-y63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_classificator_ANN(x_train, y_train, x_valid, y_valid, layers_size):\n",
        "  '''\n",
        "  Trains an artificial neural network given the labeled data.\n",
        "\n",
        "  The classificator will be created using the given sizes and activation\n",
        "  function on all the layers, except for the last one. On the last layer is\n",
        "  applied the softmax activation function.\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  x_train: ndarray\n",
        "    training set of the dataset to fit\n",
        "  y_train: ndarray\n",
        "    labels of the samples belonging to the training set\n",
        "  x_valid: ndarray\n",
        "    validation set of the dataset to fit\n",
        "  y_valid: ndarray\n",
        "    labels of the samples belonging to the validation set\n",
        "  layers_size: list \n",
        "    ordered sizes of the layers of the artificial neural network\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  parameters: list\n",
        "    parameters of the trained artificial neural network\n",
        "  history: dict\n",
        "    history of the loss functions during the training phase\n",
        "  '''\n",
        "  \n",
        "  parameters = initialize_parameters(layers_size)\n",
        "  return SGD(x_train, y_train, x_valid, y_valid, parameters)"
      ],
      "metadata": {
        "id": "3fMiuuGB7u3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Artificial Neural Network training and testing"
      ],
      "metadata": {
        "id": "ruJORmuj7xwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>ANN with one neuron in the output layer</h1>"
      ],
      "metadata": {
        "id": "CnkAEUyK3MzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_np = data_1.to_numpy()\n",
        "data_normalized_np = min_max(data_np)\n",
        "x_train, y_train, x_valid, y_valid, x_test, y_test = train_split(data_normalized_np, percentage_train=0.7, percentage_validation=0.1)\n",
        "x_train, y_train = undersample(x_train, y_train, 40.)\n",
        "\n",
        "# Definition of the model\n",
        "layers_size = [x_train.shape[1],100,30,10,1]\n",
        "\n",
        "# Training\n",
        "ANN_parameters, histories = fit_classificator_ANN(x_train, y_train, x_valid, y_valid, layers_size)\n",
        "plot_history(histories)\n",
        "\n",
        "# Testing\n",
        "y_pred_test = ANN(x_test, ANN_parameters)\n",
        "y_pred_test = y_pred_test > 0.5\n",
        "\n",
        "metrics(y_pred_test,y_test)"
      ],
      "metadata": {
        "id": "7ZR8_gDnT_cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation and testing of a model for the 5 sets of selected features\n",
        "\n",
        "datasets = [data_1, data_2, data_3, data_4, data_5, data_6, data_7]\n",
        "features_names = [v1, v2, v3, v4, v5, v6, v_random]\n",
        "metrics_dataframe = None\n",
        "ANN_parameters = list()\n",
        "histories = list()\n",
        "\n",
        "for i, dataset in enumerate(datasets):\n",
        "  # Pre-processing the dataset\n",
        "  dataset_np = dataset.to_numpy()\n",
        "  # Splitting train set, validation set and test set\n",
        "  x_train, y_train, x_valid, y_valid, x_test, y_test = train_split(dataset_np, percentage_train=0.7, percentage_validation=0.1)\n",
        "  # Normalizing the training set\n",
        "  x_train_normalized, data_train_min, data_train_max = min_max(x_train, None, None)\n",
        "  # Undersampling the training set\n",
        "  x_train_normalized, y_train = undersample(x_train_normalized, y_train, 30.)\n",
        "  \n",
        "  # Definition of the model\n",
        "  layers_size = [x_train.shape[1],200,100,100,50,1]\n",
        "\n",
        "  # Fitting the model of the ANN\n",
        "  ANN_params, history = fit_classificator_ANN(x_train, y_train, x_valid, y_valid, layers_size)\n",
        "  ANN_parameters.append(ANN_params)\n",
        "  histories.append(history)\n",
        "\n",
        "  # Getting predictions with the trained model\n",
        "  y_predicted = ANN(x_test, ANN_params)\n",
        "  y_predicted = y_predicted > 0.5\n",
        "\n",
        "  # Computing metrics\n",
        "  metrics_dataframe = metricsDF(y_predicted, y_test, metrics_df=metrics_dataframe, dataset_label='v'+str(i+1))\n",
        "  \n",
        "  fpr, tpr, _ = roc_curve(y_test, y_predicted)\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "  print(\"AUC: %.2f\" % roc_auc)\n",
        "  \n",
        "  # Plot the ROC curve\n",
        "  plt.plot(fpr, tpr, label='AUC dataset %d = %0.2f' % (i+1, roc_auc))\n",
        "  \n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.show()\n",
        "\n",
        "metrics_dataframe"
      ],
      "metadata": {
        "id": "e0-T2sPvjoBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Too much sample for computing the losses for all the training samples"
      ],
      "metadata": {
        "id": "b5ZUNYoYysa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation and testing of a model for the 5 sets of selected features with\n",
        "# undersampling of the dataset\n",
        "\n",
        "datasets = [data_1, data_2, data_3, data_4, data_5, data_6, data_7]\n",
        "features_names = [v1, v2, v3, v4, v5, v6, v_random]\n",
        "metrics_dataframe = None\n",
        "ANN_parameters = list()\n",
        "histories = list()\n",
        "\n",
        "for i, dataset in enumerate(datasets):\n",
        "  # Pre-processing the dataset\n",
        "  dataset_np = dataset.to_numpy()\n",
        "  data_normalized_np = min_max(dataset_np)\n",
        "  x_train, y_train, x_valid, y_valid, x_test, y_test = train_split(data_normalized_np, percentage_train=0.7, percentage_validation=0.1)\n",
        "  x_train, y_train = undersample(x_train, y_train, 10.)\n",
        "\n",
        "  # Definition of the model\n",
        "  layers_size = [x_train.shape[1],100,30,10,1]\n",
        "\n",
        "  # Fitting the model of the ANN\n",
        "  ANN_params, history = fit_classificator_ANN(x_train, y_train, x_valid, y_valid, layers_size)\n",
        "  ANN_parameters.append(ANN_params)\n",
        "  histories.append(history)\n",
        "\n",
        "  # Getting predictions with the trained model\n",
        "  y_predicted = ANN(x_test, ANN_params)\n",
        "  y_predicted = y_predicted > 0.5\n",
        "  \n",
        "  # Computing metrics\n",
        "  metrics_dataframe = metricsDF(y_predicted, y_test, metrics_df=metrics_dataframe, dataset_label='v'+str(i+1))\n",
        "\n",
        "  fpr, tpr, _ = roc_curve(y_test, y_predicted)\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "  print(\"AUC: %.2f\" % roc_auc)\n",
        "  \n",
        "  # Plot the ROC curve\n",
        "  plt.plot(fpr, tpr, label='AUC dataset %d = %0.2f' % (i+1, roc_auc))\n",
        "  \n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.show()\n",
        "\n",
        "metrics_dataframe"
      ],
      "metadata": {
        "id": "vBAHHeRSjoGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>ANN with two neurons in the output layer</h1>"
      ],
      "metadata": {
        "id": "CKai0lyJ4Uz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ANN with softmax in the last layer (maybe I'll create a class or a parametrized way to do it)\n",
        "def ANN(x, parameters, feature_on_columns=True):\n",
        "  '''\n",
        "  Commputes the value of the output of the artificial neural network identified\n",
        "  by its parameters given an input, considering a last layer which is the\n",
        "  softmax layer\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x: ndarray\n",
        "    input of the artificial neural network\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "  feature_on_columns: bool\n",
        "    True, if the features are on the columns of the input, False, if the\n",
        "    features are on the rows of the input\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  ndarray\n",
        "    output value of the artificial neural network\n",
        "  '''\n",
        "\n",
        "  if len(parameters)%2!=0:\n",
        "    raise Exception(\"The input parameters must be in even number\")\n",
        "  \n",
        "  if feature_on_columns:\n",
        "    layer = x.T\n",
        "  \n",
        "  num_layers = int(len(parameters)/2)+1\n",
        "  weights = parameters[0::2]\n",
        "  biases = parameters[1::2]\n",
        "\n",
        "  for i in range(num_layers-1):\n",
        "    layer = weights[i] @ layer - biases[i]\n",
        "    \n",
        "    # Activation function is applied to all the layers since the output is \n",
        "    # needed to be between 0 and 1\n",
        "    if i < num_layers - 2:\n",
        "      layer = jax.nn.sigmoid(layer)\n",
        "  \n",
        "  den = jnp.sum(jnp.exp(layer), axis = 0)\n",
        "  layer = jnp.exp(layer) / den\n",
        "\n",
        "  if feature_on_columns:\n",
        "    layer = layer.T\n",
        "  \n",
        "  return layer"
      ],
      "metadata": {
        "id": "8qGNZoOm46LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_np = data_1.to_numpy()\n",
        "data_normalized_np = min_max(data_np)\n",
        "x_train, y_train, x_valid, y_valid, x_test, y_test = train_split(data_normalized_np, percentage_train=0.7, percentage_validation=0.1)\n",
        "x_train, y_train = undersample(x_train, y_train, 30.)\n",
        "\n",
        "# Definition of the model\n",
        "layers_size = [x_train.shape[1],400,100,30,15,2]\n",
        "\n",
        "# Training\n",
        "ANN_parameters, histories = fit_classificator_ANN(x_train, y_train, x_valid, y_valid, layers_size)\n",
        "plot_history(histories)\n",
        "\n",
        "# Testing\n",
        "y_pred_test = ANN(x_test, ANN_parameters)\n",
        "y_pred_test = y_pred_test > 0.5\n",
        "\n",
        "metrics(y_pred_test,y_test)"
      ],
      "metadata": {
        "id": "fvKIFADe7ybj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes\n",
        "\n"
      ],
      "metadata": {
        "id": "V4vMDXN5UNHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition of a class representing the gaussian naive Bayes model\n",
        "class GaussianBayesModel:\n",
        "  '''\n",
        "  Gaussian naive Bayes model, binary classification\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  probability_true: float\n",
        "    probability of a sample to belong to the positive class\n",
        "  mean_features_given_true: ndarray\n",
        "    means of the features considering the samples belonging to the positive class\n",
        "  mean_features_given_false: ndarray\n",
        "    means of the features considering the samples belonging to the negative class\n",
        "  stddev_features_given_true: ndarray\n",
        "    standard deviation of the features considering the samples belonging to the positive class\n",
        "  stddev_features_given_false: ndarray\n",
        "    standard deviation of the features considering the samples belonging to the negative class\n",
        "  name_features: list\n",
        "    names of the features used in the model\n",
        "  \n",
        "  Methods\n",
        "  -------\n",
        "  plot_distributions: plots the distributions of the features of the dataset\n",
        "  predict: to make predictions using the built model\n",
        "  '''\n",
        "\n",
        "  def __init__(self, x, y, name_features, features_on_columns=True):\n",
        "    '''\n",
        "    Initializes the gaussian naive Bayes model\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    x: ndarray\n",
        "      dataset used for computing the model\n",
        "    y: ndarray\n",
        "      labels for each sample of the dataset representing the class of the sample\n",
        "    name_features: list\n",
        "      names of the features of the dataset\n",
        "    feature_on_columns: bool\n",
        "      True, if the features are on the columns of the dataset, False, if the\n",
        "      features are on the rows of the dataset\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    GaussianBayesModel\n",
        "      computed gaussian naive Bayes model\n",
        "    '''\n",
        "    \n",
        "    dataset = x\n",
        "    labels = y\n",
        "    if features_on_columns == False:\n",
        "      dataset = x.T\n",
        "    # Computing the samples belonging to the positive class\n",
        "    x_true = dataset[(y.reshape(y.shape[0])==1),:]\n",
        "    # Computing the samples belonging to the negative class\n",
        "    x_false = dataset[(y.reshape(y.shape[0])==0),:]\n",
        "    \n",
        "    # Computing the probability of a sample to belong to the positive class\n",
        "    self.probability_true = x_true.shape[0]/dataset.shape[0]\n",
        "\n",
        "    # Computing means and standard deviations of the features of the samples\n",
        "    # belonging to the positive class\n",
        "    self.mean_features_given_true = np.mean(x_true, axis=0)[None,:]\n",
        "    self.stddev_features_given_true = np.std(x_true, axis=0)[None,:]\n",
        "\n",
        "    # Computing means and standard deviations of the features of the samples\n",
        "    # belonging to the negative class\n",
        "    self.mean_features_given_false = np.mean(x_false, axis=0)[None,:]\n",
        "    self.stddev_features_given_false = np.std(x_false, axis=0)[None,:]\n",
        "    \n",
        "    # Storing the name of the features of the dataset\n",
        "    self.name_features = name_features\n",
        "\n",
        "  def __str__(self):\n",
        "    '''\n",
        "    Returns a textual representation of the gaussian naive Bayes model\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "      string representing the model\n",
        "    '''\n",
        "\n",
        "    # Difining the string representing the model as concatenation of the attributes\n",
        "    # mean_features_given_true, mean_features_given_false, stddev_features_given_true\n",
        "    # and stddev_features_given_false\n",
        "    model_string = 'Means of the features given the positiveness:\\n'+ \\\n",
        "      str(self.mean_features_given_true) + \\\n",
        "      '\\nMeans of the features given the negativeness:\\n' + \\\n",
        "      str(self.mean_features_given_false) + \\\n",
        "      '\\n Standard deviations of the features given the positiveness:\\n' + \\\n",
        "      str(self.stddev_features_given_true) + \\\n",
        "      '\\n Standard deviations of the features given the negativeness:\\n '+ \\\n",
        "      str(self.stddev_features_given_true)\n",
        "    \n",
        "    return model_string\n",
        "\n",
        "  def plot_distributions(self, label_positive='true', label_negative='false', sample=None):\n",
        "    '''\n",
        "    Plots the gaussian distributions representing the distribution of the \n",
        "    features given the classes: p(feature|label = 1), p(feature|label = 0)\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    label_positive: str\n",
        "      label of the class which is the searched positive one\n",
        "    label_negative: str\n",
        "      label of the other class which is the negative one\n",
        "    sample: ndarray\n",
        "      if not None, its features will be displyed on the plots\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    ndarray\n",
        "      ordered labels representing the classes of the given samples\n",
        "    '''\n",
        "\n",
        "    fig,axs = plt.subplots(self.mean_features_given_true.shape[1], 1, \n",
        "                           figsize=(6, self.mean_features_given_true.shape[1]*6))\n",
        "    axs=axs.flatten()\n",
        "\n",
        "    for i in range(self.mean_features_given_true.shape[1]):\n",
        "      # Taking the x values between [mean - 4 * std, mean + 4 * std] in order to\n",
        "      # plot a significant part of the bell of the gaussian distribution\n",
        "      x = np.linspace(self.mean_features_given_true[0,i] - 4 * self.stddev_features_given_true[0,i], \n",
        "          self.mean_features_given_true[0,i] + 4 * self.stddev_features_given_true[0,i], 200)\n",
        "      \n",
        "      # Plotting the gaussian distributions corresponding to the distribution of\n",
        "      # the features for the sample belonging to the positive class (fraud)\n",
        "      axs[i].plot(x, (np.exp(-(x - self.mean_features_given_true[0,i]) ** 2\n",
        "                             / (2 * self.stddev_features_given_true[0,i] ** 2))\n",
        "                      / (np.sqrt(2 * np.pi) * self.stddev_features_given_true[0,i])),\n",
        "                  label=(str(i) + ' ' if self.name_features is None \\\n",
        "                         else self.name_features[i] + ' ') + label_positive)\n",
        "      \n",
        "      # Taking the x values between [mean - 4 * std, mean + 4 * std] in order to\n",
        "      # plot a significant part of the bell of the gaussian distribution\n",
        "      x = np.linspace(self.mean_features_given_false[0,i] - 4 * self.stddev_features_given_false[0,i],\n",
        "          self.mean_features_given_false[0,i] + 4 * self.stddev_features_given_false[0,i], 200)\n",
        "      \n",
        "      # Plotting the gaussian distributions corresponding to the distribution of\n",
        "      # the features for the sample belonging to the negative class (not fraud)\n",
        "      axs[i].plot(x, (np.exp(-(x - self.mean_features_given_false[0,i]) ** 2\n",
        "                             / (2 * self.stddev_features_given_false[0,i] ** 2))\n",
        "                      / (np.sqrt(2 * np.pi) * self.stddev_features_given_false[0,i])), \n",
        "                  label=(str(i) + ' ' if self.name_features is None \\\n",
        "                         else self.name_features[i] + ' ') + label_negative)\n",
        "      \n",
        "      # Plotting a line corresponding to the values of the given sample\n",
        "      if sample is not None:\n",
        "        axs[i].axvline(x=sample[i], color='red')\n",
        "      axs[i].legend()\n",
        "\n",
        "  def predict(self, x, feature_on_columns=True, safe_prediction_parameter=1.):\n",
        "    '''\n",
        "    Predicts the classes to which belongs the given samples basing on the\n",
        "    gaussian naive Bayes model\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    x: ndarray\n",
        "      dataset composed by the samples of which the labels have to be predicted\n",
        "    feature_on_columns: bool\n",
        "      True, if the features are on the columns of the dataset, False, if the\n",
        "      features are on the rows of the dataset\n",
        "    safe_prediction_parameter: float\n",
        "      parameter for changing the relationship between the \"probability\" of the\n",
        "      positiveness and the negativeness needed for stating that a sample is\n",
        "      positive. Starting with value 1 and io\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ndarray\n",
        "      ordered labels representing the classes of the given samples\n",
        "    '''\n",
        "\n",
        "    data = x\n",
        "    if feature_on_columns == False:\n",
        "      data = x.T\n",
        "\n",
        "    # Computing probability of the given samples to belong to the positive class\n",
        "    # given their features (applying the log to all terms)\n",
        "    probability_true_given_features = np.sum(-((data - self.mean_features_given_true) ** 2)\n",
        "                                              / (2 * self.stddev_features_given_true ** 2)\n",
        "                                             - np.log(np.sqrt(2 * np.pi) * self.stddev_features_given_true), axis=1)\n",
        "    probability_true_given_features += np.log(self.probability_true)\n",
        "\n",
        "    # Computing probability of the given samples to belong to the negative class\n",
        "    # given their features (applying the log to all terms)\n",
        "    probability_false_given_features = np.sum(-((data - self.mean_features_given_false) ** 2)\n",
        "                                               / (2*self.stddev_features_given_false ** 2)\n",
        "                                              - np.log(np.sqrt(2 * np.pi) * self.stddev_features_given_false), axis=1)\n",
        "    probability_false_given_features += np.log(1 - self.probability_true)\n",
        "    \n",
        "    # Computing the prediction considering the one with the class with the gratest\n",
        "    # log-probability\n",
        "    y_pred = np.array([1 if probability_true_given_features[i] > \\\n",
        "                      safe_prediction_parameter * probability_false_given_features[i] else 0 for i in range(x.shape[0])])\n",
        "\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "3yzISV-2k5H9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fitting of the models and testing NB"
      ],
      "metadata": {
        "id": "CQ1lJI6UUNsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation and testing of a model for the 7 sets of selected features\n",
        "\n",
        "datasets = [data_1, data_2, data_3, data_4, data_5, data_6, data_7]\n",
        "features_names = [v1, v2, v3, v4, v5, v6, v_random]\n",
        "metrics_train_set = None\n",
        "metrics_valid_set = None\n",
        "metrics_test_set = None\n",
        "gnb_models = list()\n",
        "\n",
        "for i, dataset in enumerate(datasets):\n",
        "  # Pre-processing the dataset\n",
        "  dataset_np = dataset.to_numpy()\n",
        "  # Splitting train set, validation set and test set\n",
        "  x_train, y_train, x_valid, y_valid, x_test, y_test = train_split(dataset_np, percentage_train=0.7, percentage_validation=0.1)\n",
        "  # Normalizing the train set\n",
        "  x_train_normalized, data_train_min, data_train_max = min_max(x_train, None, None)\n",
        "\n",
        "  # Defining the model\n",
        "  gm = GaussianBayesModel(x_train_normalized, y_train, features_names[i])\n",
        "  gnb_models.append(gm)\n",
        "\n",
        "  # Normalizing the validation set\n",
        "  x_valid_normalized, _, _ = min_max(x_valid, data_train_min, data_train_max)\n",
        "  # Normalizing the test set\n",
        "  x_test_normalized, _, _ = min_max(x_test, data_train_min, data_train_max)\n",
        "\n",
        "  # Getting predictions with the trained model\n",
        "  y_predicted = gm.predict(x_train_normalized, safe_prediction_parameter=1)\n",
        "  # Computing metrics\n",
        "  metrics_train_set = metricsDF(y_predicted[:,None], y_train, metrics_df=metrics_train_set, dataset_label='v'+str(i+1)+' training')\n",
        "  \n",
        "  # Getting predictions with the trained model\n",
        "  y_predicted = gm.predict(x_valid_normalized, safe_prediction_parameter=1)\n",
        "  # Computing metrics\n",
        "  metrics_valid_set = metricsDF(y_predicted[:,None], y_valid, metrics_df=metrics_valid_set, dataset_label='v'+str(i+1)+' validation')\n",
        "  \n",
        "  # Getting predictions with the trained model\n",
        "  y_predicted = gm.predict(x_test_normalized, safe_prediction_parameter=1)\n",
        "  # Computing metrics\n",
        "  metrics_test_set = metricsDF(y_predicted[:,None], y_test, metrics_df=metrics_test_set, dataset_label='v'+str(i+1)+' test')\n",
        "\n",
        "  fpr, tpr, _ = roc_curve(y_test, y_predicted)\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "  print(\"AUC: %.2f\" % roc_auc)\n",
        "  \n",
        "  # Plot the ROC curve\n",
        "  plt.plot(fpr, tpr, label='AUC dataset %d = %0.2f' % (i+1, roc_auc))\n",
        "  \n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "thrxzl2k568B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the metrics for the training set\n",
        "metrics_train_set.style"
      ],
      "metadata": {
        "id": "dSgA3puC_omB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the metrics for the validation set\n",
        "metrics_valid_set.style"
      ],
      "metadata": {
        "id": "gaYZ1gR6_pp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the metrics for the test set\n",
        "metrics_test_set.style"
      ],
      "metadata": {
        "id": "COnl2CYw_puK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation and testing of a model for the 5 sets of selected features with\n",
        "# undersampling of the dataset\n",
        "\n",
        "datasets = [data_1, data_2, data_3, data_4, data_5, data_6, data_7]\n",
        "features_names = [v1, v2, v3, v4, v5, v6, v_random]\n",
        "metrics_train_set = None\n",
        "metrics_valid_set = None\n",
        "metrics_test_set = None\n",
        "gnb_models = list()\n",
        "\n",
        "for i, dataset in enumerate(datasets):\n",
        "  # Pre-processing the dataset\n",
        "  dataset_np = dataset.to_numpy()\n",
        "  # Splitting train set, validation set and test set\n",
        "  x_train, y_train, x_valid, y_valid, x_test, y_test = train_split(dataset_np, percentage_train=0.7, percentage_validation=0.1)\n",
        "  # Normalizing the training set\n",
        "  x_train_normalized, data_train_min, data_train_max = min_max(x_train, None, None)\n",
        "  # Undersampling the training set\n",
        "  x_train_normalized, y_train = undersample(x_train_normalized, y_train, 30.)\n",
        "\n",
        "  # Defining the model\n",
        "  gm = GaussianBayesModel(x_train_normalized, y_train, features_names[i])\n",
        "  gnb_models.append(gm)\n",
        "\n",
        "  # Normalizing the validation set\n",
        "  x_valid_normalized, _, _ = min_max(x_valid, data_train_min, data_train_max)\n",
        "  # Normalizing the test set\n",
        "  x_test_normalized, _, _ = min_max(x_test, data_train_min, data_train_max)\n",
        "\n",
        "  # Getting predictions with the trained model\n",
        "  y_predicted = gm.predict(x_train_normalized, safe_prediction_parameter=1)\n",
        "  # Computing metrics\n",
        "  metrics_train_set = metricsDF(y_predicted[:,None], y_train, metrics_df=metrics_train_set, dataset_label='v'+str(i+1)+' training')\n",
        "  \n",
        "  # Getting predictions with the trained model\n",
        "  y_predicted = gm.predict(x_valid_normalized, safe_prediction_parameter=1)\n",
        "  # Computing metrics\n",
        "  metrics_valid_set = metricsDF(y_predicted[:,None], y_valid, metrics_df=metrics_valid_set, dataset_label='v'+str(i+1)+' validation')\n",
        "  \n",
        "  # Getting predictions with the trained model\n",
        "  y_predicted = gm.predict(x_test_normalized, safe_prediction_parameter=1)\n",
        "  # Computing metrics\n",
        "  metrics_test_set = metricsDF(y_predicted[:,None], y_test, metrics_df=metrics_test_set, dataset_label='v'+str(i+1)+' test')\n",
        "\n",
        "  fpr, tpr, _ = roc_curve(y_test, y_predicted)\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "  print(\"AUC: %.2f\" % roc_auc)\n",
        "  \n",
        "  # Plot the ROC curve\n",
        "  plt.plot(fpr, tpr, label='AUC dataset %d = %0.2f' % (i+1, roc_auc))\n",
        "  \n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2y1fJL5tCX2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the metrics for the training set\n",
        "metrics_train_set.style"
      ],
      "metadata": {
        "id": "drJyq5TdAWTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the metrics for the validation set\n",
        "metrics_valid_set.style"
      ],
      "metadata": {
        "id": "5l32nZ6CAWV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the metrics for the test set\n",
        "metrics_test_set.style"
      ],
      "metadata": {
        "id": "QUDpmiWPAT5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods for inspecting the model"
      ],
      "metadata": {
        "id": "8VDBlDnIY1-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating and testing of the model without undersampling\n",
        "\n",
        "x_train, y_train, _, _, x_test, y_test = train_split(data_1.to_numpy(), percentage_train=0.7, percentage_validation=0.0)\n",
        "gm = GaussianBayesModel(x_train, y_train, v1)\n",
        "y_predicted = gm.predict(x_test, safe_prediction_parameter=1)\n",
        "metrics(y_predicted[:,None], y_test)"
      ],
      "metadata": {
        "id": "XIDFoOv1Z_Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating and testing of the model with undersampling\n",
        "\n",
        "x_train, y_train, _, _, x_test, y_test = train_split(data_1.to_numpy(), percentage_train=0.7, percentage_validation=0.0)\n",
        "x_train, y_train = undersample(x_train, y_train, 10.)\n",
        "gm_undersampling = GaussianBayesModel(x_train, y_train, v1)\n",
        "y_predicted = gm_undersampling.predict(x_test, safe_prediction_parameter=1)\n",
        "metrics(y_predicted[:,None], y_test)"
      ],
      "metadata": {
        "id": "A0Hi-U1TUTgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the data of the model created without undersampling\n",
        "print(gm)\n",
        "\n",
        "# Printing the data of the model created with undersampling\n",
        "print(gm_undersampling)"
      ],
      "metadata": {
        "id": "hpSYumhDePps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gm.plot_distributions(label_positive='fraud', label_negative='not fraud', sample=x_test[0,:])\n",
        "print('The transaction is not fraud') if gm.predict(x_test[0,:][None,:])[0] == 0 else print('The transaction is a fraud')"
      ],
      "metadata": {
        "id": "YWwl1eTUhmBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note: Eventualmente provare a fare la PCA per i vari datasets"
      ],
      "metadata": {
        "id": "TTzApvtP3Rl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# keep if can be useful\n",
        "# to demonstrate that the assumption of gaussian distribution of values is good\n",
        "# maybe plot for different features\n",
        "sns.histplot(x_train[:,10], kde = True)"
      ],
      "metadata": {
        "id": "KXUKXZMRKMYD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KTxDeFQ3zxAb",
        "Jio8rtx05YaI",
        "mld65YMdAmQO",
        "rFOVCtun53bg",
        "eq2mUxrLEAy4",
        "bRvGJ2sVigOT",
        "wqDFCfjD7jDR",
        "hkXbr1i1snCI",
        "dHzCLi29W6Sn",
        "5zW--vvdXnUw",
        "rTQWdgH-7UVR",
        "ruJORmuj7xwq",
        "V4vMDXN5UNHh",
        "R8hbA_NcEyOA",
        "uOv8pBsfEcaT",
        "_L-D8kxXFn9c",
        "CQ1lJI6UUNsx",
        "8VDBlDnIY1-F",
        "TTzApvtP3Rl_"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}