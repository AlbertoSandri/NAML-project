{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVzKF_-2woHf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "udowETk_skjj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dPE2EZW7QkB"
      },
      "source": [
        "Link to the [dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4A5CclDpNWB"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('drive/MyDrive/creditcard.csv', delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTxDeFQ3zxAb"
      },
      "source": [
        "## Data inspection\n",
        "\n",
        "Display some basic information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-KPQ-i3tL1s"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cy1e56i66hWo"
      },
      "outputs": [],
      "source": [
        "n_samples, n_features = data.shape\n",
        "n_frauds = np.sum(data['Class'])\n",
        "\n",
        "print('%d samples' %n_samples)\n",
        "print('%d features' %n_features)\n",
        "print('%d frauds' %n_frauds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FI-ZRGZzXZo"
      },
      "outputs": [],
      "source": [
        "data.info() # there aren't null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MV4hRT51zZ_4"
      },
      "outputs": [],
      "source": [
        "data.describe() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jio8rtx05YaI"
      },
      "source": [
        "## Attribute vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpRuQjoR09rY"
      },
      "outputs": [],
      "source": [
        "# Make the attribute vectors with the GA selected features\n",
        "v1 = ['V1', 'V5', 'V7', 'V8', 'V11', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'Amount', 'Class'] # 'Class' added\n",
        "data_1 = data[v1] \n",
        "v2 = ['V1', 'V6', 'V13', 'V16', 'V17', 'V22', 'V23', 'V28', 'Amount', 'Class']\n",
        "data_2 = data[v2]\n",
        "v3 = ['V2', 'V11', 'V12', 'V13', 'V15', 'V16', 'V17', 'V18', 'V20', 'V21', 'V24', 'V26', 'Amount', 'Class']\n",
        "data_3 = data[v3]\n",
        "v4 = ['V2', 'V7', 'V10', 'V13', 'V15', 'V17', 'V19', 'V28', 'Amount', 'Class']\n",
        "data_4 = data[v4]\n",
        "v5 = ['Time', 'V1', 'V7', 'V8', 'V9', 'V11', 'V12', 'V14', 'V15', 'V22', 'V27', 'V28', 'Amount', 'Class']\n",
        "data_5 = data[v5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E92bLkY4z9P1"
      },
      "outputs": [],
      "source": [
        "data_2.corr() # this and the following plots are not readable with the entire dataset, maybe do this just for the selected features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiUgDiXE0CU3"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots(figsize=(22,10))\n",
        "sns.heatmap(data_2.corr(), annot = True, cmap = 'vlag_r', vmin = -1, vmax = 1,  ax = ax) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6ogfUzK0hEK"
      },
      "outputs": [],
      "source": [
        "data_mean = data_2.mean()\n",
        "data_std = data_2.std()\n",
        "data_normalized = (data_2 - data_mean) / data_std # don't override data, once trained the model go back in the real world to predict there, not in the normalized world\n",
        "\n",
        "data_normalized.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOwvpClg0j5h"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots(figsize=(22,10))\n",
        "sns.violinplot(data = data_normalized, ax = ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mld65YMdAmQO"
      },
      "source": [
        "## Data normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g1KOqpoAq2l"
      },
      "source": [
        "Min-max scaling method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FL8PfA9yApo5"
      },
      "outputs": [],
      "source": [
        "def min_max(data):\n",
        "  '''\n",
        "  Applies the min-max scaling method to a numpy dataset\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  data : ndarray\n",
        "    dataset to scale with samples on rows\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  data_normalized : ndarray\n",
        "    original dataset but normalized\n",
        "  '''\n",
        "\n",
        "  data_min = data.min(axis=0)\n",
        "  data_max = data.max(axis=0)\n",
        "  data_normalized = (data - data_min[None,:]) / (data_max[None,:] - data_min[None,:]) \n",
        "  return data_normalized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFOVCtun53bg"
      },
      "source": [
        "## Train-validation split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uyw3FgyO54Lo"
      },
      "outputs": [],
      "source": [
        "# maybe use cross-validation\n",
        "# 60% train, 20% validation, 20% test, maybe it is not necessary having the validation\n",
        "\n",
        "def train_split(data_input, percentage_train=0.6, percentage_validation=0.2):\n",
        "  '''\n",
        "  Shuffles the dataset and splits in train, validation and test\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  data_input : ndarray\n",
        "    dataset to split with samples on rows\n",
        "  percentage_train : float\n",
        "    percentage of data to put in the train dataset\n",
        "  percentage_validation : float\n",
        "    percentage of data to put in the validation dataset\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  x_train : ndarray\n",
        "    train dataset with samples\n",
        "  y_train : ndarray\n",
        "    train dataset with labels\n",
        "  x_valid : ndarray\n",
        "    validation dataset with samples\n",
        "  y_valid : ndarray\n",
        "    validation dataset with labels\n",
        "  x_test : ndarray\n",
        "    test dataset with samples\n",
        "  y_test : ndarray\n",
        "    test dataset with labels\n",
        "\n",
        "  Raises\n",
        "  ------\n",
        "  Exception : if the sum of the two percentages is greater than 1 or at least \n",
        "              one of them is negative\n",
        "  '''\n",
        "\n",
        "  # check validity of the parameters\n",
        "  if percentage_train + percentage_validation > 1 or percentage_train < 0 or\\\n",
        "    percentage_validation < 0:\n",
        "    raise Exception('Percentages must be positive and their sum must be lower \\\n",
        "                    or equal to 1')\n",
        "    \n",
        "  data = data_input.copy()\n",
        "\n",
        "  np.random.seed(0) # for reproducibility\n",
        "  np.random.shuffle(data) # shuffle along first axis so rows\n",
        "\n",
        "  # take the number of samples to put in each dataset\n",
        "  num_train = int(data.shape[0] * percentage_train) \n",
        "  num_val = num_train + int(data.shape[0] * percentage_validation) \n",
        "\n",
        "  # split the dataset\n",
        "  x_train = data[:num_train, :-1] #don't take last column that is the class\n",
        "  y_train = data[:num_train, -1:]\n",
        "  x_valid = data[num_train:num_val, :-1]\n",
        "  y_valid = data[num_train:num_val, -1:]\n",
        "  x_test =  data[num_val:, :-1] \n",
        "  y_test =  data[num_val:, -1:]\n",
        "\n",
        "  return x_train, y_train, x_valid, y_valid, x_test, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Undersampling"
      ],
      "metadata": {
        "id": "eq2mUxrLEAy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# undersample the majority class, this will create a balanced dataset, allowing the classifier to better distinguish between the two classes.\n",
        "\n",
        "def undersample(x, y, ratio):\n",
        "  '''\n",
        "  Performs undersampling on the dataset\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x : ndarray\n",
        "    dataset to undersample with samples on \n",
        "  y : ndarray\n",
        "    corresponding labels of the dataset to undersample\n",
        "  ratio : float\n",
        "    desired ratio of samples between majority and minority class.\n",
        "    It assumes that label '1' is the minority class\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  x[idxs, :] : ndarray\n",
        "    dataset undersampled with samples\n",
        "  y[idxs, :] : ndarray\n",
        "    corresponding labels of the undersampled dataset\n",
        "  '''\n",
        "\n",
        "  # take indexes corresponding to labels '0' and '1'\n",
        "  idxs_zeros = np.where(y == 0.)[0]\n",
        "  idxs_ones = np.where(y == 1.)[0]\n",
        "\n",
        "  # compute the number of samples of the majority class to keep\n",
        "  n_samples = int(len(idxs_ones) * ratio)\n",
        "\n",
        "  # extract randomly the samples from the majority class\n",
        "  idxs_zeros = np.random.choice(idxs_zeros, min(n_samples, len(idxs_zeros)), replace = False)\n",
        "  \n",
        "  # extract the undersampled dataset\n",
        "  idxs = np.concatenate((idxs_zeros, idxs_ones))\n",
        "  return x[idxs, :], y[idxs, :]"
      ],
      "metadata": {
        "id": "ImeZXMgfECgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRvGJ2sVigOT"
      },
      "source": [
        "##Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfEGq0L_VXIg"
      },
      "outputs": [],
      "source": [
        "# Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise\n",
        "\n",
        "def metrics(predictions, true_labels, axis=None):\n",
        "  '''\n",
        "  Computes and print metrics TP, TN, FP, FN, AC, RC, PC, F1\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  predictions : ndarray\n",
        "    predictions of samples obtained with a model\n",
        "  true_labels : ndarray\n",
        "    true labels of the samples\n",
        "  axis : matplotlib.axes._subplots.AxesSubplot, optional\n",
        "    to print the outputs on a box in a plot\n",
        "  '''\n",
        "\n",
        "  TP = np.sum(np.logical_and(predictions == 1., true_labels == 1.)) # attacks accurately flagged as attacks\n",
        "  TN = np.sum(np.logical_and(predictions == 0., true_labels == 0.)) # normal traffic accurately flagged as normal\n",
        "  FP = np.sum(np.logical_and(predictions == 1., true_labels == 0.)) # normal traffic incorrectly flagged as attacks\n",
        "  FN = np.sum(np.logical_and(predictions == 0., true_labels == 1.)) # attacks incorrectly flagged as normal \n",
        "\n",
        "  AC = ((TN + TP) / len(predictions)) * 100 # accuracy\n",
        "  RC = (TP / (FN + TP)) * 100 # recall or sensitivity\n",
        "  PR = (TP / (FP + TP)) * 100 # precision\n",
        "  F1 = 2 * PR * RC / (PR + RC) # F1-Score\n",
        "  SP = (TN / (FP + TN)) * 100 # specificity\n",
        "\n",
        "  if axis is None:\n",
        "    # print the values\n",
        "    print('TP: %d' % TP)\n",
        "    print('TN: %d' % TN)\n",
        "    print('FP: %d' % FP)\n",
        "    print('FN: %d' % FN)\n",
        "    print('accuracy: %1.2f%%' % AC)\n",
        "    print('recall: %1.2f%%' % RC)\n",
        "    print('precision: %1.2f%%' % PR)\n",
        "    print('F1-Score: %1.2f%%' % F1)\n",
        "  else:\n",
        "    # print values on a plot\n",
        "    text = 'accuracy %1.2f%%\\nrecall: %1.2f%%\\nprecision: %1.2f%%\\nF1-Score: %1.2f%%' % (AC, RC, PR, F1)\n",
        "    axis.annotate(text, xy=(0.75, 0.75), xycoords='axes fraction',\n",
        "             bbox=dict(boxstyle=\"round,pad=0.8\", fc=(.5, 1., 1.)),\n",
        "             fontsize=14, color='black')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise\n",
        "\n",
        "def metricsDF(predictions, true_labels, axis=None, metrics_df=None, dataset_label='NotLabeled'):\n",
        "  \"\"\"\n",
        "  Computes and print metrics TP, TN, FP, FN, AC, RC, PC, F1\n",
        "  \"\"\"\n",
        "\n",
        "  TP = np.sum(np.logical_and(predictions == 1., true_labels == 1.)) # attacks accurately flagged as attacks\n",
        "  TN = np.sum(np.logical_and(predictions == 0., true_labels == 0.)) # normal traffic accurately flagged as normal\n",
        "  FP = np.sum(np.logical_and(predictions == 1., true_labels == 0.)) # normal traffic incorrectly flagged as attacks\n",
        "  FN = np.sum(np.logical_and(predictions == 0., true_labels == 1.)) # attacks incorrectly flagged as normal \n",
        "\n",
        "  AC = ((TN + TP) / len(predictions)) * 100 # accuracy\n",
        "  RC = (TP / (FN + TP)) * 100 # recall or sensitivity\n",
        "  PR = (TP / (FP + TP)) * 100 # precision\n",
        "  F1 = 2 * PR * RC / (PR + RC) # F1-Score\n",
        "  SP = (TN / (FP + TN)) * 100 # specificity\n",
        "\n",
        "  if metrics_df is None:\n",
        "    columns = ['Set of features', 'TP', 'TN', 'FP', 'FN', 'accuracy', 'recall', 'precision', 'F1-score']\n",
        "    metrics_df = pd.DataFrame([[dataset_label, TP, TN, FP, FN, AC, RC, PR, F1]], columns=columns)\n",
        "  else:\n",
        "    columns = ['Set of features', 'TP', 'TN', 'FP', 'FN', 'accuracy', 'recall', 'precision', 'F1-score']\n",
        "    metrics_df = metrics_df.append(pd.DataFrame([[dataset_label, TP, TN, FP, FN, AC, RC, PR, F1]], columns=columns))\n",
        "\n",
        "  #if axis is not None:\n",
        "  #  text = 'accuracy %1.2f%%\\nrecall: %1.2f%%\\nprecision: %1.2f%%\\nF1-Score: %1.2f%%' % (AC, RC, PR, F1)\n",
        "  #  axis.annotate(text, xy=(0.75, 0.75), xycoords='axes fraction',\n",
        "  #           bbox=dict(boxstyle=\"round,pad=0.8\", fc=(.5, 1., 1.)),\n",
        "  #           fontsize=14, color='black')\n",
        "  \n",
        "  return metrics_df"
      ],
      "metadata": {
        "id": "2LMipb1OHRj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9DA4mud_MUw"
      },
      "source": [
        "##Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KyXk2vEHYIA"
      },
      "outputs": [],
      "source": [
        "# can add more loss, like accuracy and MSE(se vuoi sotto in ANN sono definiti, ma con params = [bias, weights] e quind x = [1,x] o simile)\n",
        "# penalize more error on frauds not detected, so the model pays more attention to this minority class\n",
        "\n",
        "def cross_entropy(x, y, weights, bias):\n",
        "  '''\n",
        "  Cross entropy cost function\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x : ndarray\n",
        "    samples\n",
        "  y : ndarray\n",
        "    true labels associated to samples\n",
        "  weights : ndarray\n",
        "    weights of the model\n",
        "  bias : float\n",
        "    bias of the model\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  ndarray\n",
        "    the value of the cost function with the given input\n",
        "  '''\n",
        "\n",
        "  # compute prediction\n",
        "  y_pred = 1 / (1 + jnp.exp(-(bias + x @ weights)))\n",
        "  # compute loss\n",
        "  return - jnp.mean(2. * y * jnp.log(y_pred) + 0.5 * (1-y) * jnp.log(1-y_pred)) \n",
        "\n",
        "# gradient computation and jit compilation of the cost function\n",
        "grad = jax.grad(cross_entropy, argnums = [2,3])\n",
        "grad_jit = jax.jit(grad)\n",
        "cross_entropy_jit = jax.jit(cross_entropy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A08AjL0L_LzX"
      },
      "outputs": [],
      "source": [
        "# using vanilla SGD, so without learning rate decay and batch size doesn't work\n",
        "# tried RMSPROP, doesn't work well\n",
        "# we could add the callback\n",
        "\n",
        "def SGD(x_train, y_train, x_valid=None, y_valid = None, num_epochs=50000, learning_rate_max=1e-1, \\\n",
        "        learning_rate_min = 1e-3, learning_rate_decay = 50000, batch_size = 32):\n",
        "  '''\n",
        "  SGD method with mini-batch and learning rate decay\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x_train : ndarray\n",
        "    train dataset with samples on rows\n",
        "  y_train : ndarray\n",
        "    train labels\n",
        "  x_train : ndarray, optional\n",
        "    validation dataset with samples on rows\n",
        "  y_train : ndarray, optional\n",
        "    validation labels\n",
        "  num_epochs : int\n",
        "    number of epochs to perform (default is 50000)\n",
        "  learning_rate_max : float\n",
        "    max value of the learning rate (default is 1e-1)\n",
        "  learning_rate_min : float\n",
        "    min value of the learning rate (default is 1e-3)\n",
        "  learning_rate_decay : int\n",
        "    number of epochs to reach the learning_rate_min (default is 50000)\n",
        "  batch_size : int\n",
        "    size of the batch (default is 32)\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  weights : ndarray\n",
        "    computed weights of the model\n",
        "  bias : float\n",
        "    computed bias of the model\n",
        "  history : list\n",
        "    list with the history of the cost function values on training dataset\n",
        "  '''\n",
        "\n",
        "  # check if there is a validation dataset\n",
        "  validation = True if x_valid is not None and y_valid is not None else False\n",
        "\n",
        "  # initialize weights and bias\n",
        "  weights = np.random.randn(x_train.shape[1], 1) \n",
        "  bias = 0.0\n",
        "\n",
        "  # initialize history of the cost function\n",
        "  history = list()\n",
        "  history.append(cross_entropy_jit(x_train, y_train, weights, bias))\n",
        "  if validation:\n",
        "    history_valid = list()\n",
        "    history_valid.append(cross_entropy_jit(x_valid, y_valid, weights, bias))\n",
        "\n",
        "\n",
        "  for epoch in tqdm(range(num_epochs)):  \n",
        "    # take indexes of the batch\n",
        "    idxs = np.random.choice(x_train.shape[0], batch_size) \n",
        "\n",
        "    # compute gradient\n",
        "    grads = grad_jit(x_train[idxs,:], y_train[idxs,:], weights, bias)\n",
        "    \n",
        "    # compute learning rate value\n",
        "    learning_rate = max(learning_rate_min, learning_rate_max * (1 - epoch/learning_rate_decay))\n",
        "    \n",
        "    # update weights and bias\n",
        "    weights -= learning_rate * grads[0]\n",
        "    bias -= learning_rate * grads[1]\n",
        "\n",
        "    # compute value of the cost function on entire dataset, it is done periodically because\n",
        "    # if the dataset is big, this operation slows down too much the function\n",
        "    if epoch % 10 == 0:\n",
        "      history.append(cross_entropy_jit(x_train, y_train, weights, bias)) \n",
        "      if validation:\n",
        "        history_valid.append(cross_entropy_jit(x_valid, y_valid, weights, bias))\n",
        "\n",
        "  #fig, axs = plt.subplots(1,1,figsize=(15,10))\n",
        "  #print('train loss: %1.3e' % history[-1]) #confrontare con altre loss, es mse o accuracy\n",
        "  #axs.plot(history, label='train')\n",
        "  #if validation:\n",
        "  #  print('validation loss: %1.3e' % history_valid[-1]) #confrontare con altre loss, es mse o accuracy\n",
        "  #  axs.plot(history_valid, label='validation')\n",
        "  #axs.legend()\n",
        "\n",
        "  return weights, bias, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REyZTIE9KquG"
      },
      "outputs": [],
      "source": [
        "def predict_logistic_regression(x, weights, bias):\n",
        "  '''\n",
        "  Predicts labels using logistic regression\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x : ndarray\n",
        "    samples\n",
        "  weights : ndarray\n",
        "    weights of the logistic regression model\n",
        "  bias : float\n",
        "    bias of the logistic regression model\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  predictions : ndarray\n",
        "    array with labels of the samples predicted with the given model\n",
        "  '''\n",
        "\n",
        "  # predict output of samples with the model\n",
        "  y_pred = 1 / (1 + np.exp(-(bias + x @ weights)))\n",
        "\n",
        "  # link the predicted output to the label\n",
        "  predictions = np.array([0. if pred < 0.5 else 1. for pred in y_pred])[:, None] # by setting the threshold at 0.4 better results\n",
        "  return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLyQMU-KzPAZ"
      },
      "source": [
        "## Testing LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFbujhvbzSPj"
      },
      "outputs": [],
      "source": [
        "# test LR on a single dataset\n",
        "\n",
        "# set up the datasets\n",
        "data_np = data_5.to_numpy()\n",
        "data_normalized_np = min_max(data_np)\n",
        "x_train, y_train, _, _, x_test, y_test = train_split(data_normalized_np, percentage_train=0.7, percentage_validation=0.0)\n",
        "x_train, y_train = undersample(x_train, y_train, 50.)\n",
        "\n",
        "# train model\n",
        "weights, bias, history = SGD(x_train, y_train) \n",
        "\n",
        "# plot history of the values of loss function\n",
        "fig, axs = plt.subplots(1,1,figsize = (15,10)) \n",
        "axs.plot(history)\n",
        "axs.set_title('train loss: %1.3e' % history[-1])\n",
        "\n",
        "# get predictions with the trained model\n",
        "predictions = predict_logistic_regression(x_test, weights, bias) \n",
        "\n",
        "# compute metrics\n",
        "metrics(predictions, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mG7IRK-227eO"
      },
      "outputs": [],
      "source": [
        "# testing LR on all datasets \n",
        "datasets = [data_1, data_2, data_3, data_4, data_5]\n",
        "fig, axs = plt.subplots(5,1,figsize = (15,50)) \n",
        "\n",
        "for i, dataset in enumerate(datasets):\n",
        "  # set up the datasets\n",
        "  dataset_np = dataset.to_numpy()\n",
        "  dataset_normalized_np = min_max(dataset_np)\n",
        "  x_train, y_train, _, _, x_test, y_test = train_split(dataset_normalized_np, percentage_train=0.7, percentage_validation=0.0)\n",
        "  x_train, y_train = undersample(x_train, y_train, 20.)\n",
        "\n",
        "  # train model\n",
        "  weights, bias, history = SGD(x_train, y_train, batch_size=4) \n",
        "\n",
        "  # plot history of the values of loss function\n",
        "  axs[i].plot(history)\n",
        "  axs[i].set_title('train loss for dataset %d: %1.3e' % (i+1, history[-1]))\n",
        "\n",
        "  # get predictions with the trained model\n",
        "  predictions = predict_logistic_regression(x_test, weights, bias) \n",
        "\n",
        "  # compute metrics\n",
        "  metrics(predictions, y_test, axs[i]) \n",
        "  #works well also with 30 undersample and batch_size=8"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Decision tree"
      ],
      "metadata": {
        "id": "wqDFCfjD7jDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node: \n",
        "  '''\n",
        "  Class representing a node in a DecisionTree\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  feature : int\n",
        "    feature associated to the node\n",
        "  threshold : float\n",
        "    threshold associated to the feature\n",
        "  left : Node\n",
        "    left child of the node\n",
        "  right : Node\n",
        "    right child of the node\n",
        "  value : float\n",
        "    label associated to the node, if leaf\n",
        "\n",
        "  Methods\n",
        "  -------\n",
        "  is_leaf : tell if a Node is a leaf or not\n",
        "  '''\n",
        " \n",
        "  def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    feature : int\n",
        "      feature associated to the node (default is None)\n",
        "    threshold : float\n",
        "      threshold associated to the feature (default is None)\n",
        "    left : Node\n",
        "      left child of the node (default is None)\n",
        "    right : Node\n",
        "      right child of the node (default is None)\n",
        "    value : float\n",
        "      label associated to the node, if leaf (default is None)\n",
        "    '''\n",
        "\n",
        "    self.feature = feature\n",
        "    self.threshold = threshold\n",
        "    self.left = left\n",
        "    self.right = right\n",
        "    self.value = value\n",
        "\n",
        "  def is_leaf(self):\n",
        "    '''\n",
        "    Tells if the node is a leaf\n",
        "    '''\n",
        "\n",
        "    return self.value is not None\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "  '''\n",
        "  Class representing a decision tree\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  max_depth : int\n",
        "    max depth of a tree\n",
        "  n_features : int\n",
        "    number of features to evaluate while branching to build a tree\n",
        "  max_number_thresholds : int\n",
        "    max number of thresholds to evaluate while branching to build a tree\n",
        "  min_samples : int\n",
        "    min number of samples in a node to branch when building a tree\n",
        "  root : root of the tree\n",
        "  \n",
        "  Methods\n",
        "  -------\n",
        "  train : to train a decision tree model\n",
        "  predict : to make predictions using the built model\n",
        "  '''\n",
        "\n",
        "  def __init__(self, max_depth=10, n_features=None, max_number_thresholds=np.inf, min_samples=2):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    max_depth : int\n",
        "      max depth of a tree (default is 10)\n",
        "    n_features : int\n",
        "      number of features to evaluate while branching to build a tree (default is None)\n",
        "    max_number_thresholds : int\n",
        "      max number of thresholds to evaluate while branching to build a tree (default is np.inf)\n",
        "    min_samples : int\n",
        "      min number of samples in a node to branch when building a tree (default is 2)\n",
        "    '''\n",
        "\n",
        "    self.max_depth = max_depth \n",
        "    self.n_features = n_features \n",
        "    self.max_number_thresholds = max_number_thresholds \n",
        "    self.min_samples = min_samples  \n",
        "    self.root = None\n",
        "\n",
        "  def train(self, x, y):\n",
        "    '''\n",
        "    Builds the tree\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray\n",
        "      train dataset with samples on rows \n",
        "    y : ndarray\n",
        "      labels associated to the dataset\n",
        "    '''\n",
        "\n",
        "    if self.n_features is None or self.n_features > x.shape[1]:\n",
        "      self.n_features = x.shape[1] \n",
        "    self.root = self.build_tree(x, y)\n",
        "      \n",
        "  def build_tree(self, x, y, depth=0):\n",
        "    '''\n",
        "    Builds the tree recursively and returns the root\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray\n",
        "      train dataset with samples on rows\n",
        "    y : ndarray\n",
        "      labels associated to the dataset\n",
        "    depth : int\n",
        "      current depth of the tree\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Node\n",
        "      the root of the built tree\n",
        "    '''\n",
        "\n",
        "    num_samples, num_features = x.shape\n",
        "    num_values = len(np.unique(y)) # get number of different values in y \n",
        "\n",
        "    # check the stopping criteria, if so create a leaf node\n",
        "    if depth >= self.max_depth or num_samples < self.min_samples or num_values <= 1:\n",
        "      return Node(value = self.get_value(y))\n",
        "\n",
        "    # randomly select the features to evaluate\n",
        "    features = np.random.choice(num_features, self.n_features, replace=False)\n",
        "\n",
        "    # to select the best branch\n",
        "    max_gain = -1\n",
        "    branch_threshold= None\n",
        "    branch_feature = None\n",
        "\n",
        "    #print('split')\n",
        "    # evaluate all selected features\n",
        "    for feature in features:\n",
        "      # randomly select the thresholds to evaluate\n",
        "      thresholds = np.random.choice(np.unique(x[:, feature]), \n",
        "                                    min(self.max_number_thresholds, len(np.unique(x[:, feature]))), replace=False)\n",
        "\n",
        "      # evaluate all selected thresholds\n",
        "      for threshold in thresholds:\n",
        "        # compute the information gain\n",
        "        gain = self.gain(x[:, feature], y, threshold)\n",
        "\n",
        "        # check if it is the best gain\n",
        "        if gain > max_gain:\n",
        "          max_gain = gain\n",
        "          branch_threshold = threshold\n",
        "          branch_feature = feature\n",
        "\n",
        "    # build left branch\n",
        "    left_idxs = x[:, branch_feature] <= branch_threshold\n",
        "    left = self.build_tree(x[left_idxs, :], y[left_idxs, :], depth+1)\n",
        "\n",
        "    # build right branch\n",
        "    right_idxs = x[:, branch_feature] > branch_threshold\n",
        "    right = self.build_tree(x[right_idxs, :], y[right_idxs, :], depth+1)\n",
        "\n",
        "    return Node(branch_feature, branch_threshold, left, right)\n",
        "        \n",
        "  # TODO decide if keep this function, delete or merge with the other one\n",
        "  def gain_entropy(self, x, y, threshold): \n",
        "    '''\n",
        "    Computes the information gain using entropy\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray\n",
        "      train dataset with samples on rows\n",
        "    y : ndarray\n",
        "      labels associated to the dataset\n",
        "    threshold : float\n",
        "      threshold to divide the samples\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "      the value of the information gain\n",
        "    '''\n",
        "\n",
        "    # if one of the children is empty nothing is changing, so the gain is 0\n",
        "    if np.sum(x <= threshold) == 0 or np.sum(x > threshold) == 0:\n",
        "      return 0.\n",
        "\n",
        "    # compute the weights of the children\n",
        "    weight_left = np.sum(x <= threshold) / len(y)\n",
        "    weight_right = np.sum(x > threshold) / len(y)\n",
        "\n",
        "    # compute information gain\n",
        "    return self.entropy(y) - (weight_left * self.entropy(y[x <= threshold, :]) + weight_right * self.entropy(y[x > threshold, :]))\n",
        "\n",
        "  def entropy(self, y): # using gini would not require the logarithm, more efficient\n",
        "    '''\n",
        "    Computes the entropy of a vector of labels\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y : ndarray\n",
        "      vector of labels containing values 0. or 1. \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "      the entropy of the vector\n",
        "    '''\n",
        "\n",
        "    # compute probability of having 1. and 0.\n",
        "    p_ones = np.sum(y) / len(y)\n",
        "    p_zeros = 1. - p_ones\n",
        "\n",
        "    # compute entropy\n",
        "    # to avoid computing logarithm of 0.\n",
        "    if p_ones == 0.:\n",
        "      return - p_zeros * np.log(p_zeros)\n",
        "    if p_zeros == 0.:\n",
        "      return -  p_ones * np.log(p_ones)\n",
        "    return -(p_zeros * np.log(p_zeros) + p_ones * np.log(p_ones))\n",
        "\n",
        "  def gain(self, x, y, threshold):\n",
        "    '''\n",
        "    Computes the information gain using gini.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray\n",
        "      train dataset with samples on rows\n",
        "    y : ndarray\n",
        "      labels associated to the dataset\n",
        "    threshold : float\n",
        "      threshold to divide the samples\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "      the value of the information gain\n",
        "    '''\n",
        "\n",
        "    # take indexes of left and right children\n",
        "    left_idxs = x <= threshold\n",
        "    right_idxs = x > threshold\n",
        "\n",
        "    # if one of the children is empty nothing is changing, so the gain is 0\n",
        "    if np.sum(left_idxs) == 0 or np.sum(right_idxs) == 0:\n",
        "      return 0.\n",
        "\n",
        "    # compute the weights of the children\n",
        "    weight_left = np.sum(left_idxs) / len(y)\n",
        "    weight_right = np.sum(right_idxs) / len(y)\n",
        "\n",
        "    # compute information gain\n",
        "    return self.gini(y) - (weight_left * self.gini(y[left_idxs, :]) + weight_right * self.gini(y[right_idxs, :]))\n",
        "\n",
        "  def gini(self, y):\n",
        "    '''\n",
        "    Computes the gini index of a vector of labels\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y : ndarray\n",
        "      vector of labels containing values 0. or 1.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "      the value of the gini index\n",
        "    '''\n",
        "\n",
        "    # compute probability of having 1. and 0.\n",
        "    p_ones = np.sum(y) / len(y)\n",
        "    p_zeros = 1. - p_ones\n",
        "\n",
        "    # compute gini index\n",
        "    return 1. - (p_zeros**2 + p_ones**2)\n",
        "\n",
        "  def get_value(self, y):\n",
        "    '''\n",
        "    Finds the most common label in a vector\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y : ndarray\n",
        "      vector of labels containing values 0. or 1.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "      the value of the most common label\n",
        "    '''\n",
        "\n",
        "    if len(y) == 0: #TODO what happens if I have a leaf node with 0 elements?\n",
        "      return np.random.choice([0., 1.])\n",
        "\n",
        "    ones = np.sum(y)\n",
        "    zeros = len(y) - ones\n",
        "    return 1. if ones >= zeros else 0.\n",
        "\n",
        "  def predict(self, x):\n",
        "    '''\n",
        "    Returns the predictions on the input dataset\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray\n",
        "      test dataset with samples on rows\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ndarray\n",
        "      predicted labels\n",
        "    '''\n",
        "    \n",
        "    # compute prediction for each sample of the input\n",
        "    return np.array([self.get_prediction(sample, self.root) for sample in x])[:,None] # return a column vector\n",
        "\n",
        "  def get_prediction(self, x, node):\n",
        "    '''\n",
        "    Recursively traverse the tree to make a prediction\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray\n",
        "      sample to predict\n",
        "    node : Node\n",
        "      current node of the tree\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "      predicted label\n",
        "    '''\n",
        "\n",
        "    # if the node is a leaf return the prediction\n",
        "    if node.is_leaf():\n",
        "      return node.value\n",
        "    \n",
        "    # if the value of the sample corresponding to the feature of the node \n",
        "    # is less than the threshold of the node go left, otherwise go right\n",
        "    if x[node.feature] <= node.threshold:\n",
        "      return self.get_prediction(x, node.left)\n",
        "    else:\n",
        "      return self.get_prediction(x, node.right)"
      ],
      "metadata": {
        "id": "Unqbtpmi7m4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing DT"
      ],
      "metadata": {
        "id": "hkXbr1i1snCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test DT on a single dataset\n",
        "\n",
        "# set up the datasets\n",
        "data_np = data_4.to_numpy()\n",
        "data_normalized_np = min_max(data_np)\n",
        "x_train, y_train, _, _, x_test, y_test = train_split(data_normalized_np, percentage_train=0.7, percentage_validation=0.0)\n",
        "\n",
        "# train model\n",
        "classifier = DecisionTree(max_depth=7, n_features=4, max_number_thresholds=100, min_samples=5) # higher max_thresh performs better but takes more time to train\n",
        "classifier.train(x_train, y_train)\n",
        "\n",
        "# get predictions with the trained model\n",
        "predictions = classifier.predict(x_test)\n",
        "\n",
        "# compute metrics\n",
        "metrics(predictions, y_test) \n",
        "\n",
        "# pruning, or require a minimum number of samples per leaf per non rischiare overfitting"
      ],
      "metadata": {
        "id": "a_DLCjlL76JN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing DT on all datasets \n",
        "datasets = [data_1, data_2, data_3, data_4, data_5]\n",
        "\n",
        "for i, dataset in enumerate(datasets):\n",
        "  # set up the datasets\n",
        "  dataset_np = dataset.to_numpy()\n",
        "  dataset_normalized_np = min_max(dataset_np)\n",
        "  x_train, y_train, _, _, x_test, y_test = train_split(dataset_normalized_np, percentage_train=0.7, percentage_validation=0.0)\n",
        "  x_train, y_train = undersample(x_train, y_train, 200.)\n",
        "\n",
        "  # train model\n",
        "  classifier = DecisionTree(max_depth=7, max_number_thresholds=50) # higher max_thresh performs better but takes more time to train\n",
        "  classifier.train(x_train, y_train)\n",
        "\n",
        "  # get predictions with the trained model\n",
        "  predictions = classifier.predict(x_test)\n",
        "\n",
        "  # compute metrics\n",
        "  print(\"Metrics for dataset %d\" %(i+1))\n",
        "  metrics(predictions, y_test) "
      ],
      "metadata": {
        "id": "NHcIJqEfdm9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random forest"
      ],
      "metadata": {
        "id": "dHzCLi29W6Sn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# magari  validare gli alberi sulla restante parte del dataset e tenere solo i migliori? Non so se il risultato possa migliorare\n",
        "class RandomForest:\n",
        "  '''\n",
        "  Class representing a random forest\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  n_trees : int \n",
        "    number of trees in the forest\n",
        "  max_depth : int \n",
        "    max depth of a tree\n",
        "  n_features : int\n",
        "    number of features to evaluate while branching to build a tree\n",
        "  max_number_thresholds : int\n",
        "    max number of thresholds to evaluate while branching to build a tree\n",
        "  min_samples : int\n",
        "    min number of samples in a node to branch when building a tree\n",
        "  trees : list\n",
        "    list with all the trees\n",
        "\n",
        "  Methods\n",
        "  -------\n",
        "  train : to train a random forest model\n",
        "  predict : to make predictions using the built model\n",
        "  '''\n",
        "\n",
        "  def __init__(self, n_trees=50, max_depth=10, n_features=None, max_number_thresholds=np.inf, min_samples=2):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_trees : int\n",
        "      number of trees in the forest (default is 50)\n",
        "    max_depth : int\n",
        "      max depth of a tree (default is 10)\n",
        "    n_features : int\n",
        "      number of features to evaluate while branching to build a tree (default is None)\n",
        "    max_number_thresholds : int\n",
        "      max number of thresholds to evaluate while branching to build a tree (default is np.inf)\n",
        "    min_samples : int\n",
        "      min number of samples in a node to branch when building a tree (default is 2)\n",
        "    '''\n",
        "\n",
        "    self.n_trees = n_trees\n",
        "    self.max_depth = max_depth \n",
        "    self.n_features = n_features \n",
        "    self.max_number_thresholds = max_number_thresholds \n",
        "    self.min_samples = min_samples\n",
        "    self.trees = []\n",
        "  \n",
        "  def train(self, x, y):\n",
        "    '''\n",
        "    Builds the forest\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray\n",
        "      train dataset with samples on rows \n",
        "    y : ndarray\n",
        "      labels associated to the dataset\n",
        "    ''' \n",
        "\n",
        "    for i in tqdm(range(self.n_trees)):\n",
        "      # create bootstrapped dataset\n",
        "      n_samples = x.shape[0]\n",
        "      idxs = np.random.choice(n_samples, n_samples, replace=True) # with replacement\n",
        "      \n",
        "      # train a tree\n",
        "      tree = DecisionTree(self.max_depth, self.n_features, self.max_number_thresholds, self.min_samples)\n",
        "      tree.train(x[idxs, :], y[idxs, :])\n",
        "      self.trees.append(tree)\n",
        "\n",
        "  def predict(self, x):\n",
        "    '''\n",
        "    Returns the predictions by majority vote\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray\n",
        "      test dataset with samples on rows\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    majorities : ndarray\n",
        "      predicted labels\n",
        "    '''\n",
        "\n",
        "    # make predictions for each tree \n",
        "    votes = np.array([tree.predict(x) for tree in self.trees])\n",
        "    \n",
        "    # compute majority \n",
        "    majorities = np.mean(votes, axis=0)\n",
        "    \n",
        "    # associate the labels\n",
        "    majorities[majorities >= 0.5] = 1.\n",
        "    majorities[majorities < 0.5] = 0.\n",
        "    return majorities"
      ],
      "metadata": {
        "id": "Xz467gI3W-GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing RF"
      ],
      "metadata": {
        "id": "5zW--vvdXnUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test RF on a single dataset\n",
        "\n",
        "# set up the datasets\n",
        "data_np = data_5.to_numpy()\n",
        "data_normalized_np = min_max(data_np)\n",
        "x_train, y_train, _, _, x_test, y_test = train_split(data_normalized_np, percentage_train=0.7, percentage_validation=0.0)\n",
        "x_train, y_train = undersample(x_train, y_train, 5.)\n",
        "\n",
        "# train model\n",
        "classifier = RandomForest(n_trees=50, max_depth=7, max_number_thresholds=100) \n",
        "classifier.train(x_train, y_train)\n",
        "\n",
        "# get predictions with the trained model\n",
        "predictions = classifier.predict(x_test)\n",
        "\n",
        "# compute metrics\n",
        "metrics(predictions, y_test) \n"
      ],
      "metadata": {
        "id": "ibr4snEMXOCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing DT on all datasets \n",
        "datasets = [data_1, data_2, data_3, data_4, data_5]\n",
        "\n",
        "for i, dataset in enumerate(datasets):\n",
        "  # set up the datasets\n",
        "  dataset_np = dataset.to_numpy()\n",
        "  dataset_normalized_np = min_max(dataset_np)\n",
        "  x_train, y_train, _, _, x_test, y_test = train_split(dataset_normalized_np, percentage_train=0.7, percentage_validation=0.0)\n",
        "  x_train, y_train = undersample(x_train, y_train, 200.)\n",
        "\n",
        "  # train model\n",
        "  classifier = RandomForest(n_trees=20, max_depth=7, n_features=4, max_number_thresholds=50)  # better with 20 trees instead of 50\n",
        "  classifier.train(x_train, y_train)\n",
        "\n",
        "  # get predictions with the trained model\n",
        "  predictions = classifier.predict(x_test)\n",
        "\n",
        "  # compute metrics\n",
        "  print(\"Metrics for dataset %d\" %(i+1))\n",
        "  metrics(predictions, y_test) "
      ],
      "metadata": {
        "id": "TWl-JuujYGC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Artificial Neural Network Function Definition"
      ],
      "metadata": {
        "id": "rTQWdgH-7UVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import jax.numpy as jnp\n",
        "import jax"
      ],
      "metadata": {
        "id": "Pqz4wHlJ7ght"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(layers_size):\n",
        "  '''\n",
        "  Returns the parameters of the neural network given the number of neurons of\n",
        "  its layers.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  layers_size: list containing the ordered sizes of the layers of the neural\n",
        "  network.\n",
        "  '''\n",
        "  np.random.seed(0)\n",
        "  parameters = list()\n",
        "\n",
        "  for i in range(len(layers_size) - 1):\n",
        "    W = np.random.randn(layers_size[i+1], layers_size[i])\n",
        "    b = np.zeros((layers_size[i+1], 1))\n",
        "    parameters.append(W)\n",
        "    parameters.append(b)\n",
        "  \n",
        "  return parameters"
      ],
      "metadata": {
        "id": "0t7cHl3O7lG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parameters = initialize_parameters([10,5,1])\n",
        "#print(len(parameters))\n",
        "#print(parameters)"
      ],
      "metadata": {
        "id": "AxyifLETZ-cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ANN(x, parameters):\n",
        "  '''\n",
        "  Returns the value predicted by the neural network with given parameters.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x: input of the neural network,\n",
        "  parameters: list of trainable parameters of the neural network.\n",
        "  '''\n",
        "  if len(parameters)%2!=0:\n",
        "    raise Exception(\"The input parameters must be in even number\")\n",
        "  \n",
        "  layer = x.T\n",
        "  num_layers = int(len(parameters)/2)+1\n",
        "\n",
        "  weights = parameters[0::2]\n",
        "  biases = parameters[1::2]\n",
        "\n",
        "  for i in range(num_layers-1):\n",
        "\n",
        "    layer = weights[i] @ layer - biases[i]\n",
        "    \n",
        "    # Activation function is applied to all the layers since the output is \n",
        "    # needed to be between 0 and 1\n",
        "    layer = jax.nn.sigmoid(layer)\n",
        "\n",
        "  return layer.T"
      ],
      "metadata": {
        "id": "A8ufUeKQ7pm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#input = np.array([[1,1,1,1,1,1,1,1,1,1],[2,1,1,1,1,1,1,1,1,1]])\n",
        "#ANN(input, parameters)"
      ],
      "metadata": {
        "id": "1iRf3j4BagZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MSE(x, y, parameters):\n",
        "  '''\n",
        "  Computes the mean square error.\n",
        "  '''\n",
        "  y_pred = ANN(x, parameters)\n",
        "  return jnp.mean(jnp.square(y - y_pred))\n",
        "\n",
        "def cross_entropy(x, y, parameters):\n",
        "  '''\n",
        "  Computes the cross entropy cost function.\n",
        "  '''\n",
        "  y_pred = ANN(x, parameters)\n",
        "  return - jnp.mean(2 * y * jnp.log(y_pred) + 0.5 * (1-y) * jnp.log(1-y_pred))\n",
        "\n",
        "def cross_entropy_general(x, y, parameters):\n",
        "  # TO DO\n",
        "  '''\n",
        "  Computes the cross entropy cost function in the case of one output for each class\n",
        "  '''\n",
        "  return jnp.mean(jnp.sum(jnp.log(y_pred)))\n",
        "\n",
        "def accuracy(x, y, parameters):\n",
        "  '''\n",
        "  Compute the accuracy of the prediction.\n",
        "  '''\n",
        "  y_pred = ANN(x, parameters)\n",
        "  labels_pred = y_pred>0.4\n",
        "  return jnp.mean(y == labels_pred)"
      ],
      "metadata": {
        "id": "qzUCZimF7sSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy(input, np.array([[1],[0]]), parameters)"
      ],
      "metadata": {
        "id": "OnmDy3gncXi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computation of the gradient and jit compilation of the loss functions\n",
        "grad_jit = jax.jit(jax.grad(cross_entropy, argnums = 2))\n",
        "#grad_jit = jax.jit(jax.grad(MSE, argnums = 2))\n",
        "\n",
        "cross_entropy_jit = jax.jit(cross_entropy)\n",
        "MSE_jit = jax.jit(MSE)\n",
        "accuracy_jit = jax.jit(accuracy)"
      ],
      "metadata": {
        "id": "WXbacuT38BUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MSE_jit(input, np.array([[0.8],[0.7]]), parameters)"
      ],
      "metadata": {
        "id": "HaviTJ4adRTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dump(x, y, parameters, histories):\n",
        "  if histories == None or len(histories) == 0:\n",
        "    histories = {'Xen':[], 'MSE':[], 'acc':[]}\n",
        "  histories['Xen'].append(cross_entropy_jit(x, y, parameters))\n",
        "  histories['MSE'].append(MSE_jit(x, y, parameters))\n",
        "  histories['acc'].append(accuracy_jit(x, y, parameters))\n",
        "  return histories"
      ],
      "metadata": {
        "id": "WEF2x3bqEYgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#histories = dump(input, np.array([[0.8],[0.7]]), parameters,{})\n",
        "#print(histories)\n",
        "\n",
        "#for i in range(10):\n",
        "#  dump(input, np.array([[0.1*i],[0.09*i]]), parameters,histories)\n",
        "#histories"
      ],
      "metadata": {
        "id": "uf7Fwg5WeTHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(histories):\n",
        "  '''Plots the histories of the losses during execution'''\n",
        "  fig, axs = plt.subplots(1,len(histories))\n",
        "  axs.flatten()\n",
        "  for i, key in enumerate(histories):\n",
        "    axs[i].plot(histories[key])\n",
        "    axs[i].set_title(list(histories.keys())[i])"
      ],
      "metadata": {
        "id": "uY-Nzy-5FMy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_history(histories)"
      ],
      "metadata": {
        "id": "cmN1-ayKepaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TO DEFINE IN ORDER TO SEE WHETHER THE RESULT IS BETTER\n",
        "def SGD(x, y, parameters, learning_rate_min=1e-4,\\\n",
        "        learning_rate_max=1e-2, learning_rate_decay=10000, num_epochs=10000, \\\n",
        "        batch_size=64):\n",
        "  \"\"\"\n",
        "  Stochastic gradient descent method with mini-batch and learning rate decay.\n",
        "  \"\"\"\n",
        "  num_samples = x.shape[0]\n",
        "  velocity = [0.0 for i in range(len(parameters))]\n",
        "  grad_args = [0.0 for i in range(len(parameters))]\n",
        "\n",
        "  histories = dump(x,y,parameters,{})\n",
        "\n",
        "  for epoch in tqdm(range(num_epochs)):\n",
        "    learning_rate = max(learning_rate_min, learning_rate_max * (1 - epoch/learning_rate_decay))\n",
        "\n",
        "    for i in range(len(parameters)):\n",
        "      grad_args[i] = parameters[i] - alpha * velocity[i]\n",
        "    idxs = np.random.choice(num_samples, batch_size)\n",
        "    grads = grad_jit(x_train[idxs,:], y_train[idxs,:], grad_args)\n",
        "    for i in range(len(parameters)):\n",
        "      velocity[i] = alpha * velocity[i] + learning_rate * grads[i]\n",
        "      parameters[i] -= velocity[i]\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "      dump(x, y, parameters, histories)\n",
        "\n",
        "  return parameters, histories"
      ],
      "metadata": {
        "id": "RTLoCYThppg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NAG(x, y, parameters, learning_rate_min=1e-4,\\\n",
        "        learning_rate_max=1e-2, learning_rate_decay=50000, num_epochs=50000, \\\n",
        "        batch_size=64, alpha=0.9):\n",
        "  \"\"\"\n",
        "  Nesterov accelleration method with mini-batch and learning rate decay.\n",
        "  \"\"\"\n",
        "  num_samples = x.shape[0]\n",
        "  velocity = [0.0 for i in range(len(parameters))]\n",
        "  grad_args = [0.0 for i in range(len(parameters))]\n",
        "\n",
        "  histories = dump(x,y,parameters,{})\n",
        "\n",
        "  for epoch in tqdm(range(num_epochs)):\n",
        "    learning_rate = max(learning_rate_min, learning_rate_max * (1 - epoch/learning_rate_decay))\n",
        "\n",
        "    for i in range(len(parameters)):\n",
        "      grad_args[i] = parameters[i] - alpha * velocity[i]\n",
        "    idxs = np.random.choice(num_samples, batch_size)\n",
        "    grads = grad_jit(x_train[idxs,:], y_train[idxs,:], grad_args)\n",
        "    for i in range(len(parameters)):\n",
        "      velocity[i] = alpha * velocity[i] + learning_rate * grads[i]\n",
        "      parameters[i] -= velocity[i]\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "      dump(x, y, parameters, histories)\n",
        "\n",
        "  return parameters, histories"
      ],
      "metadata": {
        "id": "GRo18nNu-y63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_classificator_ANN(x, y, layers_size):\n",
        "  '''\n",
        "  Trains an artificial neural network given the labeled data.\n",
        "\n",
        "  The classificator will be created using the given sizes and activation\n",
        "  function on all the layers, except for the last one. On the last layer is\n",
        "  applied the softmax activation function.\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  x: dataset to fit,\n",
        "  y: labels of the samples,\n",
        "  layers_size: list containing the ordered sizes of the layers of the neural\n",
        "  network.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  parameters: parameters of the trained neural network\n",
        "  history: history of the loss function during the trining phase\n",
        "  '''\n",
        "  parameters = initialize_parameters(layers_size)\n",
        "  return NAG(x, y, parameters)"
      ],
      "metadata": {
        "id": "3fMiuuGB7u3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mgu8d9oktS3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Artificial Neural Network Training"
      ],
      "metadata": {
        "id": "ruJORmuj7xwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_np = data_1.to_numpy()\n",
        "data_normalized_np = min_max(data_np)\n",
        "x_train, y_train, _, _, x_test, y_test = train_split(data_normalized_np, percentage_train=0.7, percentage_validation=0.0)\n",
        "\n",
        "# Random undersampling of the dataset\n",
        "np.random.seed(0)\n",
        "num_true = 200\n",
        "num_false = 9800\n",
        "idx_true = list(range((y_train==1).sum()))\n",
        "idx_false = list(range((y_train==0).sum()))\n",
        "np.random.shuffle(idx_true)\n",
        "np.random.shuffle(idx_false)\n",
        "resample_true = x_train[(y_train.reshape(y_train.shape[0])==1),:][idx_true[:num_true],:]\n",
        "resample_false = x_train[(y_train.reshape(y_train.shape[0])==0),:][idx_false[:num_false],:]\n",
        "x = np.block([[resample_true],[resample_false]])\n",
        "y = np.block([[np.ones((num_true,1))],[np.zeros((num_false,1))]])\n",
        "x.shape,y.shape\n",
        "\n",
        "# Definition of the model\n",
        "layers_size = [x_train.shape[1],200,100,100,50,1]\n",
        "\n",
        "# Training\n",
        "ANN_parameters, histories = fit_classificator_ANN(x, y, layers_size)\n",
        "\n",
        "plot_history(histories)"
      ],
      "metadata": {
        "id": "7ZR8_gDnT_cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing ANN"
      ],
      "metadata": {
        "id": "a4FkbTte77RL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "y_pred_test = ANN(x_test, ANN_parameters)\n",
        "y_pred_test = y_pred_test > 0.4\n",
        "\n",
        "metrics(y_pred_test,y_test)"
      ],
      "metadata": {
        "id": "uIsX45VJgEKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes\n",
        "\n"
      ],
      "metadata": {
        "id": "V4vMDXN5UNHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Theoretical foundations\n",
        "\n",
        "Naive Bayes classifiers are a class of classifiers with theoretical foundations on the Bayes theorem.\n",
        "\n",
        "The Bayes theorem can be expressed as:\n",
        "\n",
        "$$p(y|\\mathbf{x}) = \\frac{p(y) \\cdot p(\\mathbf{x}|y)}{p(\\mathbf{x})}$$\n",
        "\n",
        "We can assume the independence of the features since the PCA has been done over the dataset. Otherwise it is possible to apply the PCA on the dataset.\n",
        "\n",
        "Assuming the independence of the features x the bayes theorem can be written as follow:\n",
        "\n",
        "$$p(y|\\mathbf{x}) = \\frac{p(y) \\cdot \\prod_{i=1}^{n}{p(x_i|y)}}{\\prod_{i=1}^{n}{p(x_i|y)}}$$\n",
        "\n",
        "The Gaussian naive Bayes classifier consider every feature $x_i$ of the dataset to have a gaussian distribution with mean the mean of the feature and with standard deviation the standard deviation of the feature."
      ],
      "metadata": {
        "id": "R8hbA_NcEyOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fitting of the model\n",
        "\n",
        "It is considered a binary classification problem.\n",
        "The training dataset, obtained from the original one, is splitted in two sets of samples basing on the class of the sample.\n",
        "The model is defined computing separately means and standard deviations of all the given features for the samples belonging to both the classes.\n",
        "\n",
        "For defining the model it is used the class GaussianNaiveBayes which contains the means and the standard deviations of the features of the dataset.\n",
        "In the initialization step the mentioned statistics are computed.\n",
        "\n",
        "The class has another method which allows to compute the prediction of the class given an unseen sample."
      ],
      "metadata": {
        "id": "uOv8pBsfEcaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianBayesModel:\n",
        "  '''Gaussian naive Bayes model, binary classification'''\n",
        "\n",
        "  def __init__(self, x, y, name_features, feature_on_columns=True):\n",
        "    '''\n",
        "    Initializes the gaussian naive Bayes model\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    x: ndarray\n",
        "      dataset used for computing the model\n",
        "    y: ndarray\n",
        "      labels for each sample of the dataset representing the class of the sample\n",
        "    name_features: list\n",
        "      names of the features of the dataset\n",
        "    feature_on_columns: bool\n",
        "      True, if the features are on the columns of the dataset, False, if the\n",
        "      features are on the rows of the dataset\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    GaussianBayesModel\n",
        "      gaussian naive Bayes model\n",
        "    '''\n",
        "\n",
        "    labels = y\n",
        "    dataset = x\n",
        "    if feature_on_columns == False:\n",
        "      dataset = x.T\n",
        "    x_true = x[(y.reshape(y.shape[0])==1),:]\n",
        "    x_false = x[(y.reshape(y.shape[0])==0),:]\n",
        "    self.probability_true = x_true.shape\n",
        "\n",
        "    self.mean_features_given_true = np.mean(x_true, axis=0)[None,:]\n",
        "    self.mean_features_given_false = np.mean(x_false, axis=0)[None,:]\n",
        "    self.stddev_features_given_true = np.std(x_true, axis=0)[None,:]\n",
        "    self.stddev_features_given_false = np.std(x_false, axis=0)[None,:]\n",
        "    self.name_features = name_features\n",
        "\n",
        "  def __str__(self):\n",
        "    '''\n",
        "    Returns a textual representation of the gaussian naive Bayes model\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "      string representing the model\n",
        "    '''\n",
        "\n",
        "    model_string = 'Means of the features given the positiveness:\\n'+ \\\n",
        "str(self.mean_features_given_true)+'\\nMeans of the features given\\\n",
        "the negativeness:\\n'+str(self.mean_features_given_false)+'\\n Standard\\\n",
        "deviations of the features given the positiveness:\\n'+str(\\\n",
        "self.stddev_features_given_true)+'\\n Standard deviations of the\\\n",
        "features given the negativeness:\\n '+str(self.stddev_features_given_true)\n",
        "    return model_string\n",
        "\n",
        "  def plot_distributions(self, label_positive='true', label_negative='false', sample=None):\n",
        "    '''\n",
        "    Plots the gaussian distributions representing the distribution of the \n",
        "    features given the classes: p(feature|label = 1), p(feature|label = 0)\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    label_positive: str\n",
        "      label of the class which is the searched positive one\n",
        "    label_negative: str\n",
        "      label of the other class which is the negative one\n",
        "    sample: ndarray\n",
        "      if not None, its features will be displyed on the plots\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    ndarray\n",
        "      ordered labels representing the classes of the given samples\n",
        "    '''\n",
        "\n",
        "    fig,axs = plt.subplots(self.mean_features_given_true.shape[1], 1, figsize=(6, self.mean_features_given_true.shape[1]*6))\n",
        "    axs=axs.flatten()\n",
        "    for i in range(self.mean_features_given_true.shape[1]):\n",
        "      \n",
        "      x = np.linspace(self.mean_features_given_true[0,i]-4*self.stddev_features_given_true[0,i],\\\n",
        "          self.mean_features_given_true[0,i]+4*self.stddev_features_given_true[0,i], 200)[None,:]\n",
        "      axs[i].plot(x.flatten(), (np.exp(-(x-self.mean_features_given_true[0,i])\\\n",
        "          **2/(2*self.stddev_features_given_true[0,i]**2))/(np.sqrt(2*np.pi)*\\\n",
        "          self.stddev_features_given_true[0,i])).flatten(), label=\\\n",
        "          (str(i)+' ' if self.name_features is None else self.name_features[i]+' ')+\\\n",
        "          label_positive)\n",
        "      x = np.linspace(self.mean_features_given_false[0,i]-4*self.stddev_features_given_false[0,i],\\\n",
        "          self.mean_features_given_false[0,i]+4*self.stddev_features_given_false[0,i], 200)[None,:]\n",
        "      axs[i].plot(x.flatten(), (np.exp(-(x-self.mean_features_given_false[0,i])\\\n",
        "          **2/(2*self.stddev_features_given_false[0,i]**2))/(np.sqrt(2*np.pi)*\\\n",
        "          self.stddev_features_given_false[0,i])).flatten(), label=\\\n",
        "          (str(i)+' ' if self.name_features is None else self.name_features[i]+' ')+\\\n",
        "          label_negative)\n",
        "      if sample is not None:\n",
        "        axs[i].axvline(x=sample[i], color='red')\n",
        "      axs[i].legend()\n",
        "\n",
        "  def predict(self, x, feature_on_columns=True, safe_prediction_parameter=1.):\n",
        "    '''\n",
        "    Predicts the classes to which belongs the given samples basing on the\n",
        "    gaussian naive Bayes model\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    x: ndarray\n",
        "      dataset composed by the samples of which the labels have to be predicted\n",
        "    feature_on_columns: bool\n",
        "      True, if the features are on the columns of the dataset, False, if the\n",
        "      features are on the rows of the dataset\n",
        "    safe_prediction_parameter: float\n",
        "      parameter for changing the relationship between the \"probability\" of the\n",
        "      positiveness and the negativeness needed for stating that a sample is\n",
        "      positive. Starting with value 1 and io\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ndarray\n",
        "      ordered labels representing the classes of the given samples\n",
        "    '''\n",
        "    \n",
        "    data = x\n",
        "    if feature_on_columns == False:\n",
        "      data = x.T\n",
        "    probability_true_given_features = np.sum(-((data -\\\n",
        "        self.mean_features_given_true)**2)/(2*self.stddev_features_given_true\\\n",
        "        **2)-np.log(np.sqrt(2*np.pi)*self.stddev_features_given_true), axis=1)\n",
        "    probability_false_given_features = np.sum(-((data -\\\n",
        "        self.mean_features_given_false)**2)/(2*self.stddev_features_given_false\\\n",
        "        **2)-np.log(np.sqrt(2*np.pi)*self.stddev_features_given_false), axis=1)\n",
        "    \n",
        "    y_pred = np.array([1 if probability_true_given_features[i] > \\\n",
        "        safe_prediction_parameter*probability_false_given_features[i] else 0 for i in range(x.shape[0])])\n",
        "\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "3yzISV-2k5H9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing the prediction\n",
        "\n",
        "The classification of an unseen samples is done appling the rule previously mentioned and so computing $p(y|\\mathbf{x})$ for both the two possible outcomes of y. The class which is more probable will be the prediction of the sample.\n"
      ],
      "metadata": {
        "id": "_L-D8kxXFn9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing NB"
      ],
      "metadata": {
        "id": "CQ1lJI6UUNsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation and testing of a model for the 5 sets of selected features\n",
        "\n",
        "datasets = [data_1, data_2, data_3, data_4, data_5]\n",
        "features_names = [v1, v2, v3, v4, v5]\n",
        "metrics_dataframe = None\n",
        "\n",
        "for i, dataset in enumerate(datasets):\n",
        "  # Pre-processing the dataset\n",
        "  dataset_np = dataset.to_numpy()\n",
        "  x_train, y_train, _, _, x_test, y_test = train_split(dataset_np, percentage_train=0.7, percentage_validation=0.0)\n",
        "\n",
        "  # Defining the model\n",
        "  gm = GaussianBayesModel(x_train, y_train, features_names[i])\n",
        "\n",
        "  # Getting predictions with the trained model\n",
        "  y_predicted = gm.predict(x_test, safe_prediction_parameter=1)\n",
        "\n",
        "  # Computing metrics\n",
        "  metrics_dataframe = metricsDF(y_predicted[:,None], y_test, metrics_df=metrics_dataframe, dataset_label='v'+str(i))\n",
        "\n",
        "metrics_dataframe"
      ],
      "metadata": {
        "id": "thrxzl2k568B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation and testing of a model for the 5 sets of selected features with\n",
        "# undersampling of the dataset\n",
        "\n",
        "datasets = [data_1, data_2, data_3, data_4, data_5]\n",
        "features_names = [v1, v2, v3, v4, v5]\n",
        "metrics_dataframe = None\n",
        "\n",
        "for i, dataset in enumerate(datasets):\n",
        "  # Pre-processing the dataset\n",
        "  dataset_np = dataset.to_numpy()\n",
        "  x_train, y_train, _, _, x_test, y_test = train_split(dataset_np, percentage_train=0.7, percentage_validation=0.0)\n",
        "  x_train, y_train = undersample(x_train, y_train, 10.)\n",
        "\n",
        "  # Defining the model\n",
        "  gm = GaussianBayesModel(x_train, y_train, features_names[i])\n",
        "\n",
        "  # Getting predictions with the trained model\n",
        "  y_predicted = gm.predict(x_test, safe_prediction_parameter=1)\n",
        "\n",
        "  # Computing metrics\n",
        "  metrics_dataframe = metricsDF(y_predicted[:,None], y_test, metrics_df=metrics_dataframe, dataset_label='v'+str(i))\n",
        "\n",
        "metrics_dataframe"
      ],
      "metadata": {
        "id": "2y1fJL5tCX2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some methods for inspecting the model"
      ],
      "metadata": {
        "id": "8VDBlDnIY1-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing of the model without undersampling\n",
        "\n",
        "x_train, y_train, _, _, x_test, y_test = train_split(data_1.to_numpy(), percentage_train=0.7, percentage_validation=0.0)\n",
        "gm = GaussianBayesModel(x_train, y_train, v1)\n",
        "y_predicted = gm.predict(x_test, safe_prediction_parameter=1)\n",
        "metrics(y_predicted[:,None], y_test)"
      ],
      "metadata": {
        "id": "XIDFoOv1Z_Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing of the model with undersampling\n",
        "\n",
        "x_train, y_train, _, _, x_test, y_test = train_split(data_1.to_numpy(), percentage_train=0.7, percentage_validation=0.0)\n",
        "x_train, y_train = undersample(x_train, y_train, 10.)\n",
        "gm_undersampling = GaussianBayesModel(x_train, y_train, v1)\n",
        "y_predicted = gm_undersampling.predict(x_test, safe_prediction_parameter=1)\n",
        "metrics(y_predicted[:,None], y_test)"
      ],
      "metadata": {
        "id": "A0Hi-U1TUTgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "NOT ALWAYS TRUE\n",
        "It is possible to see that the undersampling model gives better results in term of classification of the test dataset. All the metrics are a little bit higher in the case of the undersamples dataset and the computations in the definition of the model are fewer."
      ],
      "metadata": {
        "id": "BVZ3eGp6da1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(gm)\n",
        "print(gm_undersampling)"
      ],
      "metadata": {
        "id": "hpSYumhDePps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gm.plot_distributions(label_positive='fraud', label_negative='not fraud', sample=x_test[0,:])\n",
        "print('The transaction is not fraud') if gm.predict(x_test[0,:][None,:])[0] == 0 else print('The transaction is a fraud')"
      ],
      "metadata": {
        "id": "YWwl1eTUhmBw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KTxDeFQ3zxAb",
        "Jio8rtx05YaI",
        "mld65YMdAmQO",
        "rFOVCtun53bg",
        "bRvGJ2sVigOT",
        "c9DA4mud_MUw",
        "CLyQMU-KzPAZ",
        "4uACH317g69p",
        "4mhg0Cm1i0JM",
        "wqDFCfjD7jDR",
        "hkXbr1i1snCI",
        "AYAt6fuvW4_I",
        "dHzCLi29W6Sn"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}